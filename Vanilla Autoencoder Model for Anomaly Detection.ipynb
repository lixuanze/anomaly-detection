{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import random\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vanilla Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_input, n_bn, dp_rate, leaky_r):\n",
    "        super().__init__()\n",
    "        self.encoder_l1 = nn.Linear(in_features = n_input, out_features = 256)\n",
    "        self.encoder_l2 = nn.Linear(in_features = 256, out_features = 128)\n",
    "        self.encoder_l3 = nn.Linear(in_features = 128, out_features = 64)\n",
    "        self.encoder_l4 = nn.Linear(in_features = 64, out_features = 32)\n",
    "        self.encoder_l5 = nn.Linear(in_features = 32, out_features = n_bn)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = dp_rate)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_r, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.dropout(self.encoder_l1(x)))\n",
    "        x = self.activation(self.dropout(self.encoder_l2(x)))\n",
    "        x = self.activation(self.dropout(self.encoder_l3(x)))\n",
    "        x = self.activation(self.dropout(self.encoder_l4(x)))\n",
    "        x = self.activation(self.encoder_l5(x))\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_input, n_bn, dp_rate, leaky_r):\n",
    "        super().__init__()\n",
    "        self.decoder_l1 = nn.Linear(in_features = n_bn, out_features = 32)\n",
    "        self.decoder_l2 = nn.Linear(in_features = 32, out_features = 64)\n",
    "        self.decoder_l3 = nn.Linear(in_features = 64, out_features = 128)\n",
    "        self.decoder_l4 = nn.Linear(in_features = 128, out_features = 256)\n",
    "        self.decoder_l5 = nn.Linear(in_features = 256, out_features = n_input)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = dp_rate)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_r, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.dropout(self.decoder_l1(x)))\n",
    "        x = self.activation(self.dropout(self.decoder_l2(x)))\n",
    "        x = self.activation(self.dropout(self.decoder_l3(x)))\n",
    "        x = self.activation(self.dropout(self.decoder_l4(x)))\n",
    "        x = self.activation(self.decoder_l5(x))\n",
    "        return x\n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n_input, n_bn, dp_rate, leaky_r):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "        self.decoder = Decoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "n_input = 279\n",
    "n_bn = 16\n",
    "dp_rate = 0.2\n",
    "leaky_r = 0.01\n",
    "l_r = 0.0002\n",
    "wd_r = 0.00000\n",
    "batch_size = 500\n",
    "num_epoch = 3\n",
    "threshold = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of AutoEncoder(\n",
       "  (encoder): Encoder(\n",
       "    (encoder_l1): Linear(in_features=279, out_features=256, bias=True)\n",
       "    (encoder_l2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (encoder_l3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (encoder_l4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (encoder_l5): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (activation): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_l1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (decoder_l2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (decoder_l3): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (decoder_l4): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (decoder_l5): Linear(in_features=256, out_features=279, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (activation): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = pd.read_feather('./Processed_New_Data.ftr')[:2500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FUND_TYPE_0</th>\n",
       "      <th>FUND_TYPE_1</th>\n",
       "      <th>FUND_TYPE_2</th>\n",
       "      <th>FUND_TYPE_3</th>\n",
       "      <th>FUND_TYPE_4</th>\n",
       "      <th>FUND_TYPE_5</th>\n",
       "      <th>FUND_TYPE_6</th>\n",
       "      <th>FUND_TYPE_7</th>\n",
       "      <th>FUND_TYPE_8</th>\n",
       "      <th>FUND_TYPE_9</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAN_CODE_188</th>\n",
       "      <th>COST</th>\n",
       "      <th>AMOUNT</th>\n",
       "      <th>INTEREST_VALUE</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>TRANSACTIONCANCELLED</th>\n",
       "      <th>Trade_Month</th>\n",
       "      <th>Trade_Day</th>\n",
       "      <th>trade_year</th>\n",
       "      <th>trade_dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.0259</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-43.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-27.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499995</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>12.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.27</td>\n",
       "      <td>1.1797</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-96.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499997</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-39.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499998</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499999</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>264.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500000 rows × 241 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         FUND_TYPE_0  FUND_TYPE_1  FUND_TYPE_2  FUND_TYPE_3  FUND_TYPE_4  \\\n",
       "0                  1            0            0            0            0   \n",
       "1                  1            0            0            0            0   \n",
       "2                  1            0            0            0            0   \n",
       "3                  1            0            0            0            0   \n",
       "4                  1            0            0            0            0   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "2499995            1            0            0            0            0   \n",
       "2499996            1            0            0            0            0   \n",
       "2499997            1            0            0            0            0   \n",
       "2499998            1            0            0            0            0   \n",
       "2499999            1            0            0            0            0   \n",
       "\n",
       "         FUND_TYPE_5  FUND_TYPE_6  FUND_TYPE_7  FUND_TYPE_8  FUND_TYPE_9  ...  \\\n",
       "0                  0            0            0            0            0  ...   \n",
       "1                  0            0            0            0            0  ...   \n",
       "2                  0            0            0            0            0  ...   \n",
       "3                  0            0            0            0            0  ...   \n",
       "4                  0            0            0            0            0  ...   \n",
       "...              ...          ...          ...          ...          ...  ...   \n",
       "2499995            0            0            0            0            0  ...   \n",
       "2499996            0            0            0            0            0  ...   \n",
       "2499997            0            0            0            0            0  ...   \n",
       "2499998            0            0            0            0            0  ...   \n",
       "2499999            0            0            0            0            0  ...   \n",
       "\n",
       "         TRAN_CODE_188    COST  AMOUNT  INTEREST_VALUE  QUANTITY  \\\n",
       "0                    0   -0.29    0.00           -0.35   -0.0259   \n",
       "1                    0   -9.93    0.00            0.00    0.0000   \n",
       "2                    0  -43.29    0.00            0.00    0.0000   \n",
       "3                    0   -1.21    0.00            0.00    0.0000   \n",
       "4                    0  -27.33    0.00            0.00    0.0000   \n",
       "...                ...     ...     ...             ...       ...   \n",
       "2499995              0   12.27    0.00           12.27    1.1797   \n",
       "2499996              0    0.00  -96.39            0.00    0.0000   \n",
       "2499997              0  -39.15    0.00            0.00    0.0000   \n",
       "2499998              0   -0.40    0.00            0.00    0.0000   \n",
       "2499999              0  264.45    0.00            0.00    0.0000   \n",
       "\n",
       "         TRANSACTIONCANCELLED  Trade_Month  Trade_Day  trade_year  \\\n",
       "0                           1          2.0        1.0         0.0   \n",
       "1                           0          2.0        1.0         0.0   \n",
       "2                           0          2.0        1.0         0.0   \n",
       "3                           0          2.0        1.0         0.0   \n",
       "4                           0          2.0        1.0         0.0   \n",
       "...                       ...          ...        ...         ...   \n",
       "2499995                     0          7.0        0.0         0.0   \n",
       "2499996                     0          7.0        0.0         0.0   \n",
       "2499997                     0          7.0        0.0         0.0   \n",
       "2499998                     0          7.0        0.0         0.0   \n",
       "2499999                     0          7.0        0.0         0.0   \n",
       "\n",
       "         trade_dayofyear  \n",
       "0                   90.0  \n",
       "1                   90.0  \n",
       "2                   90.0  \n",
       "3                   90.0  \n",
       "4                   90.0  \n",
       "...                  ...  \n",
       "2499995            243.0  \n",
       "2499996            243.0  \n",
       "2499997            243.0  \n",
       "2499998            243.0  \n",
       "2499999            243.0  \n",
       "\n",
       "[2500000 rows x 241 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2443333, 279) (2443333,) (56667, 279) (56667,)\n",
      "(14660,) (11333,)\n",
      "(9774, 279) (9774,) (11334, 279) (11334,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized_data = pd.read_feather('./Processed_New_Data.ftr')[:2500000]\n",
    "# normalized_data = pd.read_feather('./normalized_one_hot_encoded_data_Charlie.ftr')\n",
    "# normalized_data = pd.read_feather('./one_hot_encoded_data_Merge_Data.ftr')[:2500000]\n",
    "normalized_data.drop([\"trade_year\",\"trade_dayofyear\"],axis=1, inplace=True)\n",
    "normalized_data[\"AMOUNT\"] = normalized_data[\"AMOUNT\"]!= 0\n",
    "normalized_data[\"INTEREST_VALUE\"] = normalized_data[\"INTEREST_VALUE\"] != 0\n",
    "normalized_data[\"COST\"] = normalized_data[\"COST\"]!= 0\n",
    "normalized_data[\"QUANTITY\"] = normalized_data[\"QUANTITY\"]!= 0\n",
    "normalized_data = normalized_data.assign(Trade_Month_1=normalized_data[\"Trade_Month\"]==1)\n",
    "normalized_data = normalized_data.assign(Trade_Month_2=normalized_data[\"Trade_Month\"]==2)\n",
    "normalized_data = normalized_data.assign(Trade_Month_3=normalized_data[\"Trade_Month\"]==3)\n",
    "normalized_data = normalized_data.assign(Trade_Month_4=normalized_data[\"Trade_Month\"]==4)\n",
    "normalized_data = normalized_data.assign(Trade_Month_5=normalized_data[\"Trade_Month\"]==5)\n",
    "normalized_data = normalized_data.assign(Trade_Month_6=normalized_data[\"Trade_Month\"]==6)\n",
    "normalized_data = normalized_data.assign(Trade_Month_7=normalized_data[\"Trade_Month\"]==7)\n",
    "normalized_data = normalized_data.assign(Trade_Month_8=normalized_data[\"Trade_Month\"]==8)\n",
    "normalized_data = normalized_data.assign(Trade_Month_9=normalized_data[\"Trade_Month\"]==9)\n",
    "normalized_data = normalized_data.assign(Trade_Month_10=normalized_data[\"Trade_Month\"]==10)\n",
    "normalized_data = normalized_data.assign(Trade_Month_11=normalized_data[\"Trade_Month\"]==11)\n",
    "normalized_data = normalized_data.assign(Trade_Month_12=normalized_data[\"Trade_Month\"]==12)\n",
    "normalized_data = normalized_data.assign(Trade_Day_1=normalized_data[\"Trade_Day\"]==1)\n",
    "normalized_data = normalized_data.assign(Trade_Day_2=normalized_data[\"Trade_Day\"]==2)\n",
    "normalized_data = normalized_data.assign(Trade_Day_3=normalized_data[\"Trade_Day\"]==3)\n",
    "normalized_data = normalized_data.assign(Trade_Day_4=normalized_data[\"Trade_Day\"]==4)\n",
    "normalized_data = normalized_data.assign(Trade_Day_5=normalized_data[\"Trade_Day\"]==5)\n",
    "normalized_data = normalized_data.assign(Trade_Day_6=normalized_data[\"Trade_Day\"]==6)\n",
    "normalized_data = normalized_data.assign(Trade_Day_7=normalized_data[\"Trade_Day\"]==7)\n",
    "normalized_data = normalized_data.assign(Trade_Day_8=normalized_data[\"Trade_Day\"]==8)\n",
    "normalized_data = normalized_data.assign(Trade_Day_9=normalized_data[\"Trade_Day\"]==9)\n",
    "normalized_data = normalized_data.assign(Trade_Day_10=normalized_data[\"Trade_Day\"]==10)\n",
    "normalized_data = normalized_data.assign(Trade_Day_11=normalized_data[\"Trade_Day\"]==11)\n",
    "normalized_data = normalized_data.assign(Trade_Day_12=normalized_data[\"Trade_Day\"]==12)\n",
    "normalized_data = normalized_data.assign(Trade_Day_13=normalized_data[\"Trade_Day\"]==13)\n",
    "normalized_data = normalized_data.assign(Trade_Day_14=normalized_data[\"Trade_Day\"]==14)\n",
    "normalized_data = normalized_data.assign(Trade_Day_15=normalized_data[\"Trade_Day\"]==15)\n",
    "normalized_data = normalized_data.assign(Trade_Day_16=normalized_data[\"Trade_Day\"]==16)\n",
    "normalized_data = normalized_data.assign(Trade_Day_17=normalized_data[\"Trade_Day\"]==17)\n",
    "normalized_data = normalized_data.assign(Trade_Day_18=normalized_data[\"Trade_Day\"]==18)\n",
    "normalized_data = normalized_data.assign(Trade_Day_19=normalized_data[\"Trade_Day\"]==19)\n",
    "normalized_data = normalized_data.assign(Trade_Day_20=normalized_data[\"Trade_Day\"]==20)\n",
    "normalized_data = normalized_data.assign(Trade_Day_21=normalized_data[\"Trade_Day\"]==21)\n",
    "normalized_data = normalized_data.assign(Trade_Day_22=normalized_data[\"Trade_Day\"]==22)\n",
    "normalized_data = normalized_data.assign(Trade_Day_23=normalized_data[\"Trade_Day\"]==23)\n",
    "normalized_data = normalized_data.assign(Trade_Day_24=normalized_data[\"Trade_Day\"]==24)\n",
    "normalized_data = normalized_data.assign(Trade_Day_25=normalized_data[\"Trade_Day\"]==25)\n",
    "normalized_data = normalized_data.assign(Trade_Day_26=normalized_data[\"Trade_Day\"]==26)\n",
    "normalized_data = normalized_data.assign(Trade_Day_27=normalized_data[\"Trade_Day\"]==27)\n",
    "normalized_data = normalized_data.assign(Trade_Day_28=normalized_data[\"Trade_Day\"]==28)\n",
    "normalized_data = normalized_data.assign(Trade_Day_29=normalized_data[\"Trade_Day\"]==29)\n",
    "normalized_data = normalized_data.assign(Trade_Day_30=normalized_data[\"Trade_Day\"]==30)\n",
    "normalized_data = normalized_data.assign(Trade_Day_31=normalized_data[\"Trade_Day\"]==31)\n",
    "normalized_data.drop([\"Trade_Day\",\"Trade_Month\"],axis=1, inplace=True)\n",
    "normalized_data = normalized_data.astype(float)\n",
    "# Data preparing\n",
    "false_index = normalized_data['TRANSACTIONCANCELLED'] == 1\n",
    "false_data = normalized_data[false_index]\n",
    "true_index = normalized_data['TRANSACTIONCANCELLED'] == 0\n",
    "true_data = normalized_data[true_index]\n",
    "\n",
    "X_true = true_data.drop('TRANSACTIONCANCELLED', axis=1).values\n",
    "y_true = true_data['TRANSACTIONCANCELLED'].values\n",
    "X_false = false_data.drop('TRANSACTIONCANCELLED', axis=1).values\n",
    "y_false = false_data['TRANSACTIONCANCELLED'].values\n",
    "\n",
    "print(X_true.shape, y_true.shape, X_false.shape, y_false.shape)\n",
    "X_true_train, X_true_val, y_true_train, y_true_val = train_test_split(X_true, y_true, test_size=0.01, random_state=42)\n",
    "X_true_val, X_true_test, y_true_val, y_true_test = train_test_split(X_true_val, y_true_val, test_size=0.4, random_state=42)\n",
    "X_false_train, X_false_val, y_false_train, y_false_val = train_test_split(X_false, y_false, test_size=0.4, random_state=42)\n",
    "X_false_val, X_false_test, y_false_val, y_false_test = train_test_split(X_false_val, y_false_val, test_size=0.5, random_state=42)\n",
    "\n",
    "print(y_true_val.shape, y_false_val.shape)\n",
    "print(X_true_test.shape, y_true_test.shape, X_false_test.shape, y_false_test.shape)\n",
    "X_true_train = torch.FloatTensor(X_true_train)\n",
    "X_false_train = torch.FloatTensor(X_false_train)\n",
    "y_true_train = torch.FloatTensor(y_true_train)\n",
    "y_false_train = torch.FloatTensor(y_false_train)\n",
    "X_true_val = torch.FloatTensor(X_true_val)\n",
    "y_true_val = torch.FloatTensor(y_true_val)\n",
    "X_false_val = torch.FloatTensor(X_false_val)\n",
    "y_false_val = torch.FloatTensor(y_false_val)\n",
    "X_true_test = torch.FloatTensor(X_true_test)\n",
    "y_true_test = torch.FloatTensor(y_true_test)\n",
    "X_false_test = torch.FloatTensor(X_false_test)\n",
    "y_false_test = torch.FloatTensor(y_false_test)\n",
    "y_true_val.numpy()[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FUND_TYPE_0</th>\n",
       "      <th>FUND_TYPE_1</th>\n",
       "      <th>FUND_TYPE_2</th>\n",
       "      <th>FUND_TYPE_3</th>\n",
       "      <th>FUND_TYPE_4</th>\n",
       "      <th>FUND_TYPE_5</th>\n",
       "      <th>FUND_TYPE_6</th>\n",
       "      <th>FUND_TYPE_7</th>\n",
       "      <th>FUND_TYPE_8</th>\n",
       "      <th>FUND_TYPE_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Trade_Day_22</th>\n",
       "      <th>Trade_Day_23</th>\n",
       "      <th>Trade_Day_24</th>\n",
       "      <th>Trade_Day_25</th>\n",
       "      <th>Trade_Day_26</th>\n",
       "      <th>Trade_Day_27</th>\n",
       "      <th>Trade_Day_28</th>\n",
       "      <th>Trade_Day_29</th>\n",
       "      <th>Trade_Day_30</th>\n",
       "      <th>Trade_Day_31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500000 rows × 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         FUND_TYPE_0  FUND_TYPE_1  FUND_TYPE_2  FUND_TYPE_3  FUND_TYPE_4  \\\n",
       "0                1.0          0.0          0.0          0.0          0.0   \n",
       "1                1.0          0.0          0.0          0.0          0.0   \n",
       "2                1.0          0.0          0.0          0.0          0.0   \n",
       "3                1.0          0.0          0.0          0.0          0.0   \n",
       "4                1.0          0.0          0.0          0.0          0.0   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "2499995          1.0          0.0          0.0          0.0          0.0   \n",
       "2499996          1.0          0.0          0.0          0.0          0.0   \n",
       "2499997          1.0          0.0          0.0          0.0          0.0   \n",
       "2499998          1.0          0.0          0.0          0.0          0.0   \n",
       "2499999          1.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "         FUND_TYPE_5  FUND_TYPE_6  FUND_TYPE_7  FUND_TYPE_8  FUND_TYPE_9  ...  \\\n",
       "0                0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1                0.0          0.0          0.0          0.0          0.0  ...   \n",
       "2                0.0          0.0          0.0          0.0          0.0  ...   \n",
       "3                0.0          0.0          0.0          0.0          0.0  ...   \n",
       "4                0.0          0.0          0.0          0.0          0.0  ...   \n",
       "...              ...          ...          ...          ...          ...  ...   \n",
       "2499995          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "2499996          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "2499997          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "2499998          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "2499999          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "\n",
       "         Trade_Day_22  Trade_Day_23  Trade_Day_24  Trade_Day_25  Trade_Day_26  \\\n",
       "0                 0.0           0.0           0.0           0.0           0.0   \n",
       "1                 0.0           0.0           0.0           0.0           0.0   \n",
       "2                 0.0           0.0           0.0           0.0           0.0   \n",
       "3                 0.0           0.0           0.0           0.0           0.0   \n",
       "4                 0.0           0.0           0.0           0.0           0.0   \n",
       "...               ...           ...           ...           ...           ...   \n",
       "2499995           0.0           0.0           0.0           0.0           0.0   \n",
       "2499996           0.0           0.0           0.0           0.0           0.0   \n",
       "2499997           0.0           0.0           0.0           0.0           0.0   \n",
       "2499998           0.0           0.0           0.0           0.0           0.0   \n",
       "2499999           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "         Trade_Day_27  Trade_Day_28  Trade_Day_29  Trade_Day_30  Trade_Day_31  \n",
       "0                 0.0           0.0           0.0           0.0           0.0  \n",
       "1                 0.0           0.0           0.0           0.0           0.0  \n",
       "2                 0.0           0.0           0.0           0.0           0.0  \n",
       "3                 0.0           0.0           0.0           0.0           0.0  \n",
       "4                 0.0           0.0           0.0           0.0           0.0  \n",
       "...               ...           ...           ...           ...           ...  \n",
       "2499995           0.0           0.0           0.0           0.0           0.0  \n",
       "2499996           0.0           0.0           0.0           0.0           0.0  \n",
       "2499997           0.0           0.0           0.0           0.0           0.0  \n",
       "2499998           0.0           0.0           0.0           0.0           0.0  \n",
       "2499999           0.0           0.0           0.0           0.0           0.0  \n",
       "\n",
       "[2500000 rows x 280 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_a_batch(x, y, batch_size):\n",
    "    length = x.shape[0]\n",
    "    start = random.randint(0, (length - batch_size))\n",
    "    return x[start : start + batch_size], y[start : start + batch_size]\n",
    "\n",
    "def get_a_batch(x, y, start, batch_size):\n",
    "    length = x.shape[0]\n",
    "    if (start + batch_size) <= length:\n",
    "        return x[start : start + batch_size], y[start : start + batch_size]\n",
    "    else:\n",
    "        return x[start : ], y[start : ]\n",
    "\n",
    "def binary_acc(y_hat, y, threshold):\n",
    "    t = nn.Threshold(threshold, 0)\n",
    "    y_pred = t(y_hat)\n",
    "    y_pred = torch.ceil(y_pred)\n",
    "    correct_results_sum = (y_pred == y).sum().float()\n",
    "    acc = correct_results_sum/y.shape[0] \n",
    "    return acc\n",
    "\n",
    "def get_precision_and_recall(y_hat, y, threshold):\n",
    "    t = nn.Threshold(threshold, 0)\n",
    "    y_pred = t(y_hat)\n",
    "    y_pred = torch.ceil(y_pred)\n",
    "    y_pred = torch.squeeze(y_pred)\n",
    "    \n",
    "    num_tp = (y_pred + y == 2).sum().float()\n",
    "    num_fp = (y_pred - y == 1).sum().float()\n",
    "    num_tn = (y_pred + y == 0).sum().float()\n",
    "    num_fn = (y_pred - y == -1).sum().float()\n",
    "    \n",
    "    precision = num_tp / (num_tp + num_fp)\n",
    "    recall = num_tp / (num_tp + num_fn)\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0618, grad_fn=<MeanBackward0>) +  tensor(0.0603, grad_fn=<MeanBackward0>) +  tensor(0.0594, grad_fn=<MeanBackward0>) +  tensor(0.0587, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.080 VAL: loss 0.059\n",
      "TRN:\n",
      " +  tensor(0.0576, grad_fn=<MeanBackward0>) +  tensor(0.0581, grad_fn=<MeanBackward0>) +  tensor(0.0501, grad_fn=<MeanBackward0>) +  tensor(0.0442, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.052 VAL: loss 0.043\n",
      "TRN:\n",
      " +  tensor(0.0414, grad_fn=<MeanBackward0>) +  tensor(0.0411, grad_fn=<MeanBackward0>) +  tensor(0.0395, grad_fn=<MeanBackward0>) +  tensor(0.0384, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.040 VAL: loss 0.038\n"
     ]
    }
   ],
   "source": [
    "# actual training, First 3 epochs\n",
    "\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0375, grad_fn=<MeanBackward0>) +  tensor(0.0364, grad_fn=<MeanBackward0>) +  tensor(0.0364, grad_fn=<MeanBackward0>) +  tensor(0.0355, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.036 VAL: loss 0.035\n",
      "TRN:\n",
      " +  tensor(0.0346, grad_fn=<MeanBackward0>) +  tensor(0.0338, grad_fn=<MeanBackward0>) +  tensor(0.0342, grad_fn=<MeanBackward0>) +  tensor(0.0330, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.034 VAL: loss 0.033\n",
      "TRN:\n",
      " +  tensor(0.0328, grad_fn=<MeanBackward0>) +  tensor(0.0330, grad_fn=<MeanBackward0>) +  tensor(0.0329, grad_fn=<MeanBackward0>) +  tensor(0.0314, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.032 VAL: loss 0.032\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0310, grad_fn=<MeanBackward0>) +  tensor(0.0313, grad_fn=<MeanBackward0>) +  tensor(0.0315, grad_fn=<MeanBackward0>) +  tensor(0.0302, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.031 VAL: loss 0.030\n",
      "TRN:\n",
      " +  tensor(0.0294, grad_fn=<MeanBackward0>) +  tensor(0.0310, grad_fn=<MeanBackward0>) +  tensor(0.0302, grad_fn=<MeanBackward0>) +  tensor(0.0293, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.030 VAL: loss 0.029\n",
      "TRN:\n",
      " +  tensor(0.0289, grad_fn=<MeanBackward0>) +  tensor(0.0293, grad_fn=<MeanBackward0>) +  tensor(0.0299, grad_fn=<MeanBackward0>) +  tensor(0.0283, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.029 VAL: loss 0.028\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0274, grad_fn=<MeanBackward0>) +  tensor(0.0294, grad_fn=<MeanBackward0>) +  tensor(0.0286, grad_fn=<MeanBackward0>) +  tensor(0.0276, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.028 VAL: loss 0.028\n",
      "TRN:\n",
      " +  tensor(0.0270, grad_fn=<MeanBackward0>) +  tensor(0.0283, grad_fn=<MeanBackward0>) +  tensor(0.0280, grad_fn=<MeanBackward0>) +  tensor(0.0271, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.027 VAL: loss 0.027\n",
      "TRN:\n",
      " +  tensor(0.0268, grad_fn=<MeanBackward0>) +  tensor(0.0275, grad_fn=<MeanBackward0>) +  tensor(0.0275, grad_fn=<MeanBackward0>) +  tensor(0.0270, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.027 VAL: loss 0.027\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0264, grad_fn=<MeanBackward0>) +  tensor(0.0273, grad_fn=<MeanBackward0>) +  tensor(0.0271, grad_fn=<MeanBackward0>) +  tensor(0.0265, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.027 VAL: loss 0.027\n",
      "TRN:\n",
      " +  tensor(0.0265, grad_fn=<MeanBackward0>) +  tensor(0.0272, grad_fn=<MeanBackward0>) +  tensor(0.0270, grad_fn=<MeanBackward0>) +  tensor(0.0263, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.026 VAL: loss 0.026\n",
      "TRN:\n",
      " +  tensor(0.0257, grad_fn=<MeanBackward0>) +  tensor(0.0269, grad_fn=<MeanBackward0>) +  tensor(0.0265, grad_fn=<MeanBackward0>) +  tensor(0.0259, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.026 VAL: loss 0.026\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2468978, 279) (2468978,) (31022, 279) (31022,)\n",
      "(14814,) (6204,)\n",
      "(9876, 279) (9876,) (6205, 279) (6205,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized_data = pd.read_feather('./Processed_New_Data.ftr')[:2500000]\n",
    "# normalized_data = pd.read_feather('./normalized_one_hot_encoded_data_Charlie.ftr')\n",
    "normalized_data = pd.read_feather('./Processed_New_Data.ftr')[2500000:5000000]\n",
    "normalized_data.drop([\"trade_year\",\"trade_dayofyear\"],axis=1, inplace=True)\n",
    "normalized_data[\"AMOUNT\"] = normalized_data[\"AMOUNT\"]!= 0\n",
    "normalized_data[\"INTEREST_VALUE\"] = normalized_data[\"INTEREST_VALUE\"] != 0\n",
    "normalized_data[\"COST\"] = normalized_data[\"COST\"]!= 0\n",
    "normalized_data[\"QUANTITY\"] = normalized_data[\"QUANTITY\"]!= 0\n",
    "normalized_data = normalized_data.assign(Trade_Month_1=normalized_data[\"Trade_Month\"]==1)\n",
    "normalized_data = normalized_data.assign(Trade_Month_2=normalized_data[\"Trade_Month\"]==2)\n",
    "normalized_data = normalized_data.assign(Trade_Month_3=normalized_data[\"Trade_Month\"]==3)\n",
    "normalized_data = normalized_data.assign(Trade_Month_4=normalized_data[\"Trade_Month\"]==4)\n",
    "normalized_data = normalized_data.assign(Trade_Month_5=normalized_data[\"Trade_Month\"]==5)\n",
    "normalized_data = normalized_data.assign(Trade_Month_6=normalized_data[\"Trade_Month\"]==6)\n",
    "normalized_data = normalized_data.assign(Trade_Month_7=normalized_data[\"Trade_Month\"]==7)\n",
    "normalized_data = normalized_data.assign(Trade_Month_8=normalized_data[\"Trade_Month\"]==8)\n",
    "normalized_data = normalized_data.assign(Trade_Month_9=normalized_data[\"Trade_Month\"]==9)\n",
    "normalized_data = normalized_data.assign(Trade_Month_10=normalized_data[\"Trade_Month\"]==10)\n",
    "normalized_data = normalized_data.assign(Trade_Month_11=normalized_data[\"Trade_Month\"]==11)\n",
    "normalized_data = normalized_data.assign(Trade_Month_12=normalized_data[\"Trade_Month\"]==12)\n",
    "normalized_data = normalized_data.assign(Trade_Day_1=normalized_data[\"Trade_Day\"]==1)\n",
    "normalized_data = normalized_data.assign(Trade_Day_2=normalized_data[\"Trade_Day\"]==2)\n",
    "normalized_data = normalized_data.assign(Trade_Day_3=normalized_data[\"Trade_Day\"]==3)\n",
    "normalized_data = normalized_data.assign(Trade_Day_4=normalized_data[\"Trade_Day\"]==4)\n",
    "normalized_data = normalized_data.assign(Trade_Day_5=normalized_data[\"Trade_Day\"]==5)\n",
    "normalized_data = normalized_data.assign(Trade_Day_6=normalized_data[\"Trade_Day\"]==6)\n",
    "normalized_data = normalized_data.assign(Trade_Day_7=normalized_data[\"Trade_Day\"]==7)\n",
    "normalized_data = normalized_data.assign(Trade_Day_8=normalized_data[\"Trade_Day\"]==8)\n",
    "normalized_data = normalized_data.assign(Trade_Day_9=normalized_data[\"Trade_Day\"]==9)\n",
    "normalized_data = normalized_data.assign(Trade_Day_10=normalized_data[\"Trade_Day\"]==10)\n",
    "normalized_data = normalized_data.assign(Trade_Day_11=normalized_data[\"Trade_Day\"]==11)\n",
    "normalized_data = normalized_data.assign(Trade_Day_12=normalized_data[\"Trade_Day\"]==12)\n",
    "normalized_data = normalized_data.assign(Trade_Day_13=normalized_data[\"Trade_Day\"]==13)\n",
    "normalized_data = normalized_data.assign(Trade_Day_14=normalized_data[\"Trade_Day\"]==14)\n",
    "normalized_data = normalized_data.assign(Trade_Day_15=normalized_data[\"Trade_Day\"]==15)\n",
    "normalized_data = normalized_data.assign(Trade_Day_16=normalized_data[\"Trade_Day\"]==16)\n",
    "normalized_data = normalized_data.assign(Trade_Day_17=normalized_data[\"Trade_Day\"]==17)\n",
    "normalized_data = normalized_data.assign(Trade_Day_18=normalized_data[\"Trade_Day\"]==18)\n",
    "normalized_data = normalized_data.assign(Trade_Day_19=normalized_data[\"Trade_Day\"]==19)\n",
    "normalized_data = normalized_data.assign(Trade_Day_20=normalized_data[\"Trade_Day\"]==20)\n",
    "normalized_data = normalized_data.assign(Trade_Day_21=normalized_data[\"Trade_Day\"]==21)\n",
    "normalized_data = normalized_data.assign(Trade_Day_22=normalized_data[\"Trade_Day\"]==22)\n",
    "normalized_data = normalized_data.assign(Trade_Day_23=normalized_data[\"Trade_Day\"]==23)\n",
    "normalized_data = normalized_data.assign(Trade_Day_24=normalized_data[\"Trade_Day\"]==24)\n",
    "normalized_data = normalized_data.assign(Trade_Day_25=normalized_data[\"Trade_Day\"]==25)\n",
    "normalized_data = normalized_data.assign(Trade_Day_26=normalized_data[\"Trade_Day\"]==26)\n",
    "normalized_data = normalized_data.assign(Trade_Day_27=normalized_data[\"Trade_Day\"]==27)\n",
    "normalized_data = normalized_data.assign(Trade_Day_28=normalized_data[\"Trade_Day\"]==28)\n",
    "normalized_data = normalized_data.assign(Trade_Day_29=normalized_data[\"Trade_Day\"]==29)\n",
    "normalized_data = normalized_data.assign(Trade_Day_30=normalized_data[\"Trade_Day\"]==30)\n",
    "normalized_data = normalized_data.assign(Trade_Day_31=normalized_data[\"Trade_Day\"]==31)\n",
    "normalized_data.drop([\"Trade_Day\",\"Trade_Month\"],axis=1, inplace=True)\n",
    "normalized_data = normalized_data.astype(float)\n",
    "# Data preparing\n",
    "false_index = normalized_data['TRANSACTIONCANCELLED'] == 1\n",
    "false_data = normalized_data[false_index]\n",
    "true_index = normalized_data['TRANSACTIONCANCELLED'] == 0\n",
    "true_data = normalized_data[true_index]\n",
    "\n",
    "X_true = true_data.drop('TRANSACTIONCANCELLED', axis=1).values\n",
    "y_true = true_data['TRANSACTIONCANCELLED'].values\n",
    "X_false = false_data.drop('TRANSACTIONCANCELLED', axis=1).values\n",
    "y_false = false_data['TRANSACTIONCANCELLED'].values\n",
    "\n",
    "print(X_true.shape, y_true.shape, X_false.shape, y_false.shape)\n",
    "X_true_train, X_true_val, y_true_train, y_true_val = train_test_split(X_true, y_true, test_size=0.01, random_state=42)\n",
    "X_true_val, X_true_test, y_true_val, y_true_test = train_test_split(X_true_val, y_true_val, test_size=0.4, random_state=42)\n",
    "X_false_train, X_false_val, y_false_train, y_false_val = train_test_split(X_false, y_false, test_size=0.4, random_state=42)\n",
    "X_false_val, X_false_test, y_false_val, y_false_test = train_test_split(X_false_val, y_false_val, test_size=0.5, random_state=42)\n",
    "\n",
    "print(y_true_val.shape, y_false_val.shape)\n",
    "print(X_true_test.shape, y_true_test.shape, X_false_test.shape, y_false_test.shape)\n",
    "X_true_train = torch.FloatTensor(X_true_train)\n",
    "X_false_train = torch.FloatTensor(X_false_train)\n",
    "y_true_train = torch.FloatTensor(y_true_train)\n",
    "y_false_train = torch.FloatTensor(y_false_train)\n",
    "X_true_val = torch.FloatTensor(X_true_val)\n",
    "y_true_val = torch.FloatTensor(y_true_val)\n",
    "X_false_val = torch.FloatTensor(X_false_val)\n",
    "y_false_val = torch.FloatTensor(y_false_val)\n",
    "X_true_test = torch.FloatTensor(X_true_test)\n",
    "y_true_test = torch.FloatTensor(y_true_test)\n",
    "X_false_test = torch.FloatTensor(X_false_test)\n",
    "y_false_test = torch.FloatTensor(y_false_test)\n",
    "y_true_val.numpy()[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0347, grad_fn=<MeanBackward0>) +  tensor(0.0306, grad_fn=<MeanBackward0>) +  tensor(0.0292, grad_fn=<MeanBackward0>) +  tensor(0.0295, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.033 VAL: loss 0.029\n",
      "TRN:\n",
      " +  tensor(0.0294, grad_fn=<MeanBackward0>) +  tensor(0.0282, grad_fn=<MeanBackward0>) +  tensor(0.0273, grad_fn=<MeanBackward0>) +  tensor(0.0283, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.028 VAL: loss 0.028\n",
      "TRN:\n",
      " +  tensor(0.0287, grad_fn=<MeanBackward0>) +  tensor(0.0268, grad_fn=<MeanBackward0>) +  tensor(0.0271, grad_fn=<MeanBackward0>) +  tensor(0.0274, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.027 VAL: loss 0.027\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0279, grad_fn=<MeanBackward0>) +  tensor(0.0264, grad_fn=<MeanBackward0>) +  tensor(0.0261, grad_fn=<MeanBackward0>) +  tensor(0.0268, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.027 VAL: loss 0.026\n",
      "TRN:\n",
      " +  tensor(0.0269, grad_fn=<MeanBackward0>) +  tensor(0.0259, grad_fn=<MeanBackward0>) +  tensor(0.0258, grad_fn=<MeanBackward0>) +  tensor(0.0266, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.026 VAL: loss 0.026\n",
      "TRN:\n",
      " +  tensor(0.0277, grad_fn=<MeanBackward0>) +  tensor(0.0256, grad_fn=<MeanBackward0>) +  tensor(0.0255, grad_fn=<MeanBackward0>) +  tensor(0.0261, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.026 VAL: loss 0.026\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0271, grad_fn=<MeanBackward0>) +  tensor(0.0253, grad_fn=<MeanBackward0>) +  tensor(0.0253, grad_fn=<MeanBackward0>) +  tensor(0.0260, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.026 VAL: loss 0.026\n",
      "TRN:\n",
      " +  tensor(0.0266, grad_fn=<MeanBackward0>) +  tensor(0.0251, grad_fn=<MeanBackward0>) +  tensor(0.0259, grad_fn=<MeanBackward0>) +  tensor(0.0255, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.025 VAL: loss 0.025\n",
      "TRN:\n",
      " +  tensor(0.0260, grad_fn=<MeanBackward0>) +  tensor(0.0256, grad_fn=<MeanBackward0>) +  tensor(0.0251, grad_fn=<MeanBackward0>) +  tensor(0.0255, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.025 VAL: loss 0.025\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0259, grad_fn=<MeanBackward0>) +  tensor(0.0247, grad_fn=<MeanBackward0>) +  tensor(0.0246, grad_fn=<MeanBackward0>) +  tensor(0.0251, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.025 VAL: loss 0.025\n",
      "TRN:\n",
      " +  tensor(0.0260, grad_fn=<MeanBackward0>) +  tensor(0.0248, grad_fn=<MeanBackward0>) +  tensor(0.0244, grad_fn=<MeanBackward0>) +  tensor(0.0249, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.025 VAL: loss 0.025\n",
      "TRN:\n",
      " +  tensor(0.0256, grad_fn=<MeanBackward0>) +  tensor(0.0240, grad_fn=<MeanBackward0>) +  tensor(0.0248, grad_fn=<MeanBackward0>) +  tensor(0.0250, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.025 VAL: loss 0.025\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0254, grad_fn=<MeanBackward0>) +  tensor(0.0242, grad_fn=<MeanBackward0>) +  tensor(0.0239, grad_fn=<MeanBackward0>) +  tensor(0.0244, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.025 VAL: loss 0.024\n",
      "TRN:\n",
      " +  tensor(0.0253, grad_fn=<MeanBackward0>) +  tensor(0.0240, grad_fn=<MeanBackward0>) +  tensor(0.0236, grad_fn=<MeanBackward0>) +  tensor(0.0245, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.024 VAL: loss 0.024\n",
      "TRN:\n",
      " +  tensor(0.0252, grad_fn=<MeanBackward0>) +  tensor(0.0247, grad_fn=<MeanBackward0>) +  tensor(0.0235, grad_fn=<MeanBackward0>) +  tensor(0.0243, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.024 VAL: loss 0.024\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2812452, 279) (2812452,) (38543, 279) (38543,)\n",
      "(16875,) (7709,)\n",
      "(11250, 279) (11250,) (7709, 279) (7709,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized_data = pd.read_feather('./Processed_New_Data.ftr')[:2500000]\n",
    "# normalized_data = pd.read_feather('./normalized_one_hot_encoded_data_Charlie.ftr')\n",
    "normalized_data = pd.read_feather('./Processed_New_Data.ftr')[5000000:]\n",
    "normalized_data.drop([\"trade_year\",\"trade_dayofyear\"],axis=1, inplace=True)\n",
    "normalized_data[\"AMOUNT\"] = normalized_data[\"AMOUNT\"]!= 0\n",
    "normalized_data[\"INTEREST_VALUE\"] = normalized_data[\"INTEREST_VALUE\"] != 0\n",
    "normalized_data[\"COST\"] = normalized_data[\"COST\"]!= 0\n",
    "normalized_data[\"QUANTITY\"] = normalized_data[\"QUANTITY\"]!= 0\n",
    "normalized_data = normalized_data.assign(Trade_Month_1=normalized_data[\"Trade_Month\"]==1)\n",
    "normalized_data = normalized_data.assign(Trade_Month_2=normalized_data[\"Trade_Month\"]==2)\n",
    "normalized_data = normalized_data.assign(Trade_Month_3=normalized_data[\"Trade_Month\"]==3)\n",
    "normalized_data = normalized_data.assign(Trade_Month_4=normalized_data[\"Trade_Month\"]==4)\n",
    "normalized_data = normalized_data.assign(Trade_Month_5=normalized_data[\"Trade_Month\"]==5)\n",
    "normalized_data = normalized_data.assign(Trade_Month_6=normalized_data[\"Trade_Month\"]==6)\n",
    "normalized_data = normalized_data.assign(Trade_Month_7=normalized_data[\"Trade_Month\"]==7)\n",
    "normalized_data = normalized_data.assign(Trade_Month_8=normalized_data[\"Trade_Month\"]==8)\n",
    "normalized_data = normalized_data.assign(Trade_Month_9=normalized_data[\"Trade_Month\"]==9)\n",
    "normalized_data = normalized_data.assign(Trade_Month_10=normalized_data[\"Trade_Month\"]==10)\n",
    "normalized_data = normalized_data.assign(Trade_Month_11=normalized_data[\"Trade_Month\"]==11)\n",
    "normalized_data = normalized_data.assign(Trade_Month_12=normalized_data[\"Trade_Month\"]==12)\n",
    "normalized_data = normalized_data.assign(Trade_Day_1=normalized_data[\"Trade_Day\"]==1)\n",
    "normalized_data = normalized_data.assign(Trade_Day_2=normalized_data[\"Trade_Day\"]==2)\n",
    "normalized_data = normalized_data.assign(Trade_Day_3=normalized_data[\"Trade_Day\"]==3)\n",
    "normalized_data = normalized_data.assign(Trade_Day_4=normalized_data[\"Trade_Day\"]==4)\n",
    "normalized_data = normalized_data.assign(Trade_Day_5=normalized_data[\"Trade_Day\"]==5)\n",
    "normalized_data = normalized_data.assign(Trade_Day_6=normalized_data[\"Trade_Day\"]==6)\n",
    "normalized_data = normalized_data.assign(Trade_Day_7=normalized_data[\"Trade_Day\"]==7)\n",
    "normalized_data = normalized_data.assign(Trade_Day_8=normalized_data[\"Trade_Day\"]==8)\n",
    "normalized_data = normalized_data.assign(Trade_Day_9=normalized_data[\"Trade_Day\"]==9)\n",
    "normalized_data = normalized_data.assign(Trade_Day_10=normalized_data[\"Trade_Day\"]==10)\n",
    "normalized_data = normalized_data.assign(Trade_Day_11=normalized_data[\"Trade_Day\"]==11)\n",
    "normalized_data = normalized_data.assign(Trade_Day_12=normalized_data[\"Trade_Day\"]==12)\n",
    "normalized_data = normalized_data.assign(Trade_Day_13=normalized_data[\"Trade_Day\"]==13)\n",
    "normalized_data = normalized_data.assign(Trade_Day_14=normalized_data[\"Trade_Day\"]==14)\n",
    "normalized_data = normalized_data.assign(Trade_Day_15=normalized_data[\"Trade_Day\"]==15)\n",
    "normalized_data = normalized_data.assign(Trade_Day_16=normalized_data[\"Trade_Day\"]==16)\n",
    "normalized_data = normalized_data.assign(Trade_Day_17=normalized_data[\"Trade_Day\"]==17)\n",
    "normalized_data = normalized_data.assign(Trade_Day_18=normalized_data[\"Trade_Day\"]==18)\n",
    "normalized_data = normalized_data.assign(Trade_Day_19=normalized_data[\"Trade_Day\"]==19)\n",
    "normalized_data = normalized_data.assign(Trade_Day_20=normalized_data[\"Trade_Day\"]==20)\n",
    "normalized_data = normalized_data.assign(Trade_Day_21=normalized_data[\"Trade_Day\"]==21)\n",
    "normalized_data = normalized_data.assign(Trade_Day_22=normalized_data[\"Trade_Day\"]==22)\n",
    "normalized_data = normalized_data.assign(Trade_Day_23=normalized_data[\"Trade_Day\"]==23)\n",
    "normalized_data = normalized_data.assign(Trade_Day_24=normalized_data[\"Trade_Day\"]==24)\n",
    "normalized_data = normalized_data.assign(Trade_Day_25=normalized_data[\"Trade_Day\"]==25)\n",
    "normalized_data = normalized_data.assign(Trade_Day_26=normalized_data[\"Trade_Day\"]==26)\n",
    "normalized_data = normalized_data.assign(Trade_Day_27=normalized_data[\"Trade_Day\"]==27)\n",
    "normalized_data = normalized_data.assign(Trade_Day_28=normalized_data[\"Trade_Day\"]==28)\n",
    "normalized_data = normalized_data.assign(Trade_Day_29=normalized_data[\"Trade_Day\"]==29)\n",
    "normalized_data = normalized_data.assign(Trade_Day_30=normalized_data[\"Trade_Day\"]==30)\n",
    "normalized_data = normalized_data.assign(Trade_Day_31=normalized_data[\"Trade_Day\"]==31)\n",
    "normalized_data.drop([\"Trade_Day\",\"Trade_Month\"],axis=1, inplace=True)\n",
    "normalized_data = normalized_data.astype(float)\n",
    "# Data preparing\n",
    "false_index = normalized_data['TRANSACTIONCANCELLED'] == 1\n",
    "false_data = normalized_data[false_index]\n",
    "true_index = normalized_data['TRANSACTIONCANCELLED'] == 0\n",
    "true_data = normalized_data[true_index]\n",
    "\n",
    "X_true = true_data.drop('TRANSACTIONCANCELLED', axis=1).values\n",
    "y_true = true_data['TRANSACTIONCANCELLED'].values\n",
    "X_false = false_data.drop('TRANSACTIONCANCELLED', axis=1).values\n",
    "y_false = false_data['TRANSACTIONCANCELLED'].values\n",
    "\n",
    "print(X_true.shape, y_true.shape, X_false.shape, y_false.shape)\n",
    "X_true_train, X_true_val, y_true_train, y_true_val = train_test_split(X_true, y_true, test_size=0.01, random_state=42)\n",
    "X_true_val, X_true_test, y_true_val, y_true_test = train_test_split(X_true_val, y_true_val, test_size=0.4, random_state=42)\n",
    "X_false_train, X_false_val, y_false_train, y_false_val = train_test_split(X_false, y_false, test_size=0.4, random_state=42)\n",
    "X_false_val, X_false_test, y_false_val, y_false_test = train_test_split(X_false_val, y_false_val, test_size=0.5, random_state=42)\n",
    "\n",
    "print(y_true_val.shape, y_false_val.shape)\n",
    "print(X_true_test.shape, y_true_test.shape, X_false_test.shape, y_false_test.shape)\n",
    "X_true_train = torch.FloatTensor(X_true_train)\n",
    "X_false_train = torch.FloatTensor(X_false_train)\n",
    "y_true_train = torch.FloatTensor(y_true_train)\n",
    "y_false_train = torch.FloatTensor(y_false_train)\n",
    "X_true_val = torch.FloatTensor(X_true_val)\n",
    "y_true_val = torch.FloatTensor(y_true_val)\n",
    "X_false_val = torch.FloatTensor(X_false_val)\n",
    "y_false_val = torch.FloatTensor(y_false_val)\n",
    "X_true_test = torch.FloatTensor(X_true_test)\n",
    "y_true_test = torch.FloatTensor(y_true_test)\n",
    "X_false_test = torch.FloatTensor(X_false_test)\n",
    "y_false_test = torch.FloatTensor(y_false_test)\n",
    "y_true_val.numpy()[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0322, grad_fn=<MeanBackward0>) +  tensor(0.0285, grad_fn=<MeanBackward0>) +  tensor(0.0271, grad_fn=<MeanBackward0>) +  tensor(0.0273, grad_fn=<MeanBackward0>) +  tensor(0.0265, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.030 VAL: loss 0.026\n",
      "TRN:\n",
      " +  tensor(0.0257, grad_fn=<MeanBackward0>) +  tensor(0.0248, grad_fn=<MeanBackward0>) +  tensor(0.0248, grad_fn=<MeanBackward0>) +  tensor(0.0249, grad_fn=<MeanBackward0>) +  tensor(0.0260, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.025 VAL: loss 0.024\n",
      "TRN:\n",
      " +  tensor(0.0247, grad_fn=<MeanBackward0>) +  tensor(0.0246, grad_fn=<MeanBackward0>) +  tensor(0.0237, grad_fn=<MeanBackward0>) +  tensor(0.0243, grad_fn=<MeanBackward0>) +  tensor(0.0247, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.024 VAL: loss 0.024\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0236, grad_fn=<MeanBackward0>) +  tensor(0.0236, grad_fn=<MeanBackward0>) +  tensor(0.0234, grad_fn=<MeanBackward0>) +  tensor(0.0236, grad_fn=<MeanBackward0>) +  tensor(0.0242, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.024 VAL: loss 0.023\n",
      "TRN:\n",
      " +  tensor(0.0240, grad_fn=<MeanBackward0>) +  tensor(0.0234, grad_fn=<MeanBackward0>) +  tensor(0.0246, grad_fn=<MeanBackward0>) +  tensor(0.0242, grad_fn=<MeanBackward0>) +  tensor(0.0242, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.023 VAL: loss 0.023\n",
      "TRN:\n",
      " +  tensor(0.0235, grad_fn=<MeanBackward0>) +  tensor(0.0230, grad_fn=<MeanBackward0>) +  tensor(0.0235, grad_fn=<MeanBackward0>) +  tensor(0.0232, grad_fn=<MeanBackward0>) +  tensor(0.0242, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.023 VAL: loss 0.023\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0230, grad_fn=<MeanBackward0>) +  tensor(0.0227, grad_fn=<MeanBackward0>) +  tensor(0.0231, grad_fn=<MeanBackward0>) +  tensor(0.0234, grad_fn=<MeanBackward0>) +  tensor(0.0239, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.023 VAL: loss 0.023\n",
      "TRN:\n",
      " +  tensor(0.0235, grad_fn=<MeanBackward0>) +  tensor(0.0227, grad_fn=<MeanBackward0>) +  tensor(0.0225, grad_fn=<MeanBackward0>) +  tensor(0.0229, grad_fn=<MeanBackward0>) +  tensor(0.0234, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.023 VAL: loss 0.023\n",
      "TRN:\n",
      " +  tensor(0.0229, grad_fn=<MeanBackward0>) +  tensor(0.0235, grad_fn=<MeanBackward0>) +  tensor(0.0233, grad_fn=<MeanBackward0>) +  tensor(0.0233, grad_fn=<MeanBackward0>) +  tensor(0.0236, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.023 VAL: loss 0.022\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0226, grad_fn=<MeanBackward0>) +  tensor(0.0222, grad_fn=<MeanBackward0>) +  tensor(0.0224, grad_fn=<MeanBackward0>) +  tensor(0.0229, grad_fn=<MeanBackward0>) +  tensor(0.0231, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.022 VAL: loss 0.023\n",
      "TRN:\n",
      " +  tensor(0.0236, grad_fn=<MeanBackward0>) +  tensor(0.0223, grad_fn=<MeanBackward0>) +  tensor(0.0219, grad_fn=<MeanBackward0>) +  tensor(0.0225, grad_fn=<MeanBackward0>) +  tensor(0.0231, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.022 VAL: loss 0.022\n",
      "TRN:\n",
      " +  tensor(0.0226, grad_fn=<MeanBackward0>) +  tensor(0.0221, grad_fn=<MeanBackward0>) +  tensor(0.0223, grad_fn=<MeanBackward0>) +  tensor(0.0226, grad_fn=<MeanBackward0>) +  tensor(0.0233, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.022 VAL: loss 0.022\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRN:\n",
      " +  tensor(0.0220, grad_fn=<MeanBackward0>) +  tensor(0.0217, grad_fn=<MeanBackward0>) +  tensor(0.0223, grad_fn=<MeanBackward0>) +  tensor(0.0223, grad_fn=<MeanBackward0>) +  tensor(0.0230, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00000 - TRN: loss 0.022 VAL: loss 0.022\n",
      "TRN:\n",
      " +  tensor(0.0221, grad_fn=<MeanBackward0>) +  tensor(0.0216, grad_fn=<MeanBackward0>) +  tensor(0.0219, grad_fn=<MeanBackward0>) +  tensor(0.0222, grad_fn=<MeanBackward0>) +  tensor(0.0231, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00001 - TRN: loss 0.022 VAL: loss 0.022\n",
      "TRN:\n",
      " +  tensor(0.0221, grad_fn=<MeanBackward0>) +  tensor(0.0214, grad_fn=<MeanBackward0>) +  tensor(0.0218, grad_fn=<MeanBackward0>) +  tensor(0.0224, grad_fn=<MeanBackward0>) +  tensor(0.0229, grad_fn=<MeanBackward0>)VAL:\n",
      "\n",
      "\n",
      "Ep.00002 - TRN: loss 0.022 VAL: loss 0.022\n"
     ]
    }
   ],
   "source": [
    "# actual training, later epochs\n",
    "# tracing the loss along the way\n",
    "meta_info = {}\n",
    "meta_info['trn_loss'] = []\n",
    "meta_info['trn_acc'] = []\n",
    "meta_info['val_loss'] = []\n",
    "meta_info['val_acc'] = []\n",
    "iteration = 0\n",
    "\n",
    "num_trn = X_true_train.shape[0]\n",
    "\n",
    "\n",
    "model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r, weight_decay=wd_r)\n",
    "checkpoint = torch.load(\"autoencoder_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for ep in range(num_epoch):\n",
    "\n",
    "    trn_batch_start = 0\n",
    "    val_batch_start = 0\n",
    "\n",
    "    avg_trn_loss = 0\n",
    "    avg_trn_acc = 0\n",
    "\n",
    "    print('TRN:', flush=True)\n",
    "    i = 0\n",
    "    while (trn_batch_start <= num_trn - 1): \n",
    "        i += 1\n",
    "        # loop to iterate over training set\n",
    "        X_trn_batch, y_trn_batch = get_a_batch(X_true_train, y_true_train, trn_batch_start, batch_size)\n",
    "\n",
    "        iteration += 1\n",
    "        trn_batch_start += batch_size\n",
    "\n",
    "        # forward and backward propagation\n",
    "        y_hat = model.forward(X_trn_batch)\n",
    "        loss = criterion(y_hat, X_trn_batch)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(' + ', loss, end='', flush=True)\n",
    "\n",
    "        '''if i % 10 == 0:\n",
    "            print(f'Loop: {i} Loss: {loss}')'''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_trn_loss = avg_trn_loss + loss\n",
    "\n",
    "\n",
    "    avg_trn_loss = avg_trn_loss*batch_size/num_trn\n",
    "\n",
    "    #Validation\n",
    "    print('VAL:', flush=True)\n",
    "\n",
    "    y_val_hat = model.forward(X_true_val)\n",
    "    val_loss = criterion(y_val_hat, X_true_val)\n",
    "    avg_val_loss = torch.mean(val_loss)\n",
    "\n",
    "    print('', flush=True)\n",
    "\n",
    "    print('\\nEp.%05d - TRN: loss %.3f VAL: loss %.3f' % (ep, avg_trn_loss, avg_val_loss), flush=True)\n",
    "\n",
    "\n",
    "    # save learning meta information\n",
    "    meta_info['trn_loss'].append(avg_trn_loss)\n",
    "    meta_info['val_loss'].append(avg_val_loss)\n",
    "torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"autoencoder_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Distribution & Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normal_distribution(mu, var, weight):\n",
    "    sigma = math.sqrt(var)\n",
    "    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 1000)\n",
    "    plt.plot(x, stats.norm.pdf(x, mu, sigma) * weight)\n",
    "    \n",
    "def solve_intersection(m1, m2, var1, var2, w1, w2):\n",
    "    std1 = np.sqrt(var1)\n",
    "    std2 = np.sqrt(var2)\n",
    "    a = -1/var1 + 1/var2\n",
    "    b = 2*(m1/var1 - m2/var2)\n",
    "    c = m2**2/var2 - m1**2/var1 + np.log(var2/var1)\n",
    "    c = c + (2 * np.log(w1/w2))\n",
    "    return np.roots([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Encoder(\n",
       "    (encoder_l1): Linear(in_features=279, out_features=256, bias=True)\n",
       "    (encoder_l2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (encoder_l3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (encoder_l4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (encoder_l5): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (activation): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_l1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (decoder_l2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (decoder_l3): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (decoder_l4): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (decoder_l5): Linear(in_features=256, out_features=279, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (activation): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = AutoEncoder(n_input, n_bn, dp_rate, leaky_r)\n",
    "test_model.load_state_dict(torch.load(\"autoencoder_model.pt\"))\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01586468 0.02066929 0.02314373 ... 0.02817382 0.03989647 0.01086546] [0.02356471 0.03335049 0.03973782 ... 0.03926381 0.01086546 0.02711763]\n",
      "(23125,)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "x_test_true, _ = random_a_batch(X_true_train, y_true_train, X_false_train.shape[0])\n",
    "y_true_hat = test_model.forward(x_test_true)\n",
    "true_loss = criterion(y_true_hat, x_test_true)\n",
    "true_loss = torch.mean(true_loss, dim=1)\n",
    "true_loss = true_loss.detach().numpy()\n",
    "\n",
    "y_false_hat = test_model.forward(X_false_train)\n",
    "false_loss = criterion(y_false_hat, X_false_train)\n",
    "false_loss = torch.mean(false_loss, dim=1)\n",
    "false_loss = false_loss.detach().numpy()\n",
    "\n",
    "print(true_loss, false_loss)\n",
    "print(true_loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff4987671f0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3iV9fn48fedkBAgCYEQRlhh76VMcaAiAuJGxVFRHKWOWlvbr9W2X+3PfmuttWqtW8SBqMVZRUVx4ECRLXsGCIEkJGQRQtb9++N5giEm5CQ5O/frus51xrPuHA7nPp8tqooxxhjjqYhAB2CMMSa0WOIwxhhTL5Y4jDHG1IslDmOMMfViicMYY0y9WOIwxhhTL5Y4jFeIyJMi8kcvnaubiBSKSKT7/HMRud4b53bP94GIzPTW+epx3ftE5ICI7PfhNQpFpKe3961nDPeIyMvePq8JHs0CHYAJfiKSCnQAyoByYAPwIvC0qlYAqOrsepzrelX9pLZ9VHU3ENu4qI9e7x6gt6peVeX8U7xx7nrG0RX4DdBdVTNr2D4BeFlVuzTmOqrq8ftWn32NqcpKHMZT56pqHNAduB/4H+A5b19ERML1x0x3ILumpOGpMH5vTIixxGHqRVXzVPVd4DJgpogMBhCRuSJyn/u4nYi8JyK5IpIjIl+KSISIvAR0A/7rVpP8TkRSRERF5DoR2Q18WuW1ql+UvURkmYjkicg7ItLWvdYEEUmrGqOIpIrIRBGZDNwFXOZeb427/WjVlxvXH0Rkl4hkisiLItLa3VYZx0wR2e1WM91d23sjIq3d47Pc8/3BPf9E4GMg2Y1jbrXjWgEfVNleKCLJbpXPAhF5WUTygWtEZLSILHXf230i8piIRFc5l4pI7yr/Jv8WkfdFpEBEvhORXg3cd5KIbHbf/8dF5AtPqw9F5DwRWe/G/LmIDKiy7X9EZK97zc0icqb7+mgRWS4i+SKSISIPeXIt4x+WOEyDqOoyIA04pYbNv3G3JeFUcd3lHKI/A3bjlF5iVfWBKsecBgwAzq7lklcDs4BknCqzRz2I8UPg/4DX3OsNq2G3a9zb6UBPnCqyx6rtczLQDzgT+FPVL75q/gW0ds9zmhvztW613BQg3Y3jmmpxHqq2PVZV093N5wMLgARgHk5V4e1AO2CcG9NNx3kbLgfuBdoA24C/1HdfEWnnxvB7IBHYDJx0nPMcJSJ9gfnAr3A+DwtxfjhEi0g/4BZglFuaPRtIdQ99BHhEVeOBXsDrnlzP+IclDtMY6UDbGl4vBTrh1OeXquqXWvekaPeo6iFVPVzL9pdUdZ37JftH4FJxG88b6UrgIVXdoaqFOF+OM6qVdu5V1cOqugZYA/wkAbmxXAb8XlULVDUV+Afws0bGt1RV31bVCjeGFar6raqWudd4CidJ1eZNVV2mqmU4iWd4A/adCqxX1TfdbY8CnjbwXwa8r6ofq2op8CDQAifxlAPNgYEiEqWqqaq63T2uFOgtIu1UtVBVv/XwesYPLHGYxugM5NTw+t9xfrEuEpEdInKnB+faU4/tu4AonF/djZXsnq/quZvhlJQqVf2SLKLmhvt2QHQN5+rcyPiOeV9EpK9bDbjfrb76P47/PngSe137JleNw/0RcEz14HEc8/66nSn2AJ1VdRtOSeQeIFNEXhWRZHfX64C+wCYR+V5Epnl4PeMHljhMg4jIKJwvxa+qb3N/cf9GVXsC5wK/rqy7BmoredRVIula5XE3nF+kB4BDQMsqcUXiVIl4et50nIbrqucuAzLqOK66A25M1c+118PjPX1fngA2AX3capy7AKlHnA2xDzja20tEpOrzOhzz/rrHdsV9X1T1FVU92d1Hgb+5r29V1cuB9u5rC9y2IBMELHGYehGRePfX36s43Ud/qGGfaSLS2/2SyMepkih3N2fgtAHU11UiMlBEWgJ/BhaoajmwBYgRkXNEJAr4A071R6UMIEVEavuszwduF5EeIhLLj20iZfUJzo3ldeAvIhInIt2BXwOejmfIABIrG+aPIw7nPS0Ukf7AL+oTZwO9DwwRkQvcKrybgY4eHvs6cI6InOn++/wGOAJ8IyL9ROQMEWkOFAOHcT8nInKViCS5JZRc91zlNZzfBIAlDuOp/4pIAU41w93AQ8C1tezbB/gEKASWAo+r6ufutr8Cf3B72NxRj+u/BMzFqU6JAX4JTi8vnMbhZ3F+xR7i2GqU/7j32SKysobzznHPvQTYifMFdms94qrqVvf6O3BKYq+456+Tqm7CSWI73PcmuZZd7wCuAAqAZ4DXGhirx1T1AHAJ8ACQDQwEluMkgLqO3QxchdNx4ABOCfRcVS3BSfD3u6/vxyld3OUeOhlYLyKFOA3lM1S12It/lmkEsYWcjDH14Zbe0oArVfWzQMdj/M9KHMaYOonI2SKS4FYrVbarWE+nJsoShzHGE+OA7fxY3XTBcbpOmzBnVVXGGGPqxUocxhhj6sVnk6aJMxvoizjd9ipwZlJ9RJw5hl4DUnCmF7hUVQ/WcPxknN4UkcCzqnp/Xdds166dpqSkeOtPMMaYsLdixYoDqppU954/8llVlYh0Ajqp6koRiQNWABfgzAuUo6r3uyOK26jq/1Q7NhKnf/5ZOL03vgcuV9UNx7vmyJEjdfny5d7/Y4wxJkyJyApVHVmfY3xWVaWq+1R1pfu4ANiIM9L4fOAFd7cXcJJJdaOBbe78QSU4g83O91WsxhhjPOeXNg4RSQFGAN8BHVR1HzjJBWfQT3WdOXaOnjRqmfNHRG50p19enpWV5c2wjTHG1MDnicOdxuEN4Feqmu/pYTW8VmOdmqo+raojVXVkUlK9qumMMcY0gE9XFHPnpnkDmKeqb7ovZ4hIJ1Xd57aD1LQiWhrHTmrXBWeyNGNME1daWkpaWhrFxTYDSX3ExMTQpUsXoqKiGn0uX/aqEpylRTeqatXVu94FZuLMUTMTeKeGw78H+ohID5z5h2bgzM9jjGni0tLSiIuLIyUlBedrxtRFVcnOziYtLY0ePXo0+ny+rKoaj7OIzRkistq9TcVJGGeJyFacXlP3A4izVOZCAHdm0luAj3Aa1V9X1fU+jNUYEyKKi4tJTEy0pFEPIkJiYqLXSmk+K3Go6lfUvk7AmdVfcJfKnFrl+UKcZSaNMeYYljTqz5vvmU/bOIwJViVlFXy4fj9pB4sY1zOREd3aBDokY0KGTTlimpysgiNMf/Ibfjl/FQ98uJkLH/+G/1u4EZu3zQSL3NxcHn/88aPP09PTmT59egAjOpYlDtOklJRV8IuXV7Alo4DHrhjB6j+dxZVjuvH0kh08vWRHoMMzBvhp4khOTmbBggUBjOhYljhMkzLn650s33WQv108lGlDk0loGc19Fwxm8qCO/GPRFrZmFAQ6RBMCUlNTGTBgADfccAODBg1i0qRJHD58mO3btzN58mROPPFETjnlFDZt2gTA9u3bGTt2LKNGjeJPf/oTsbGxABQWFnLmmWdywgknMGTIEN55x+lkeuedd7J9+3aGDx/Ob3/7W1JTUxk8eDAAY8aMYf36H/sKTZgwgRUrVnDo0CFmzZrFqFGjGDFixNFz+YK1cZgm40DhER77dBtn9m/P+cN/nIhARLjvwsGc/uDn/P2jzTx9db2m7TEBdO9/17Mh3dNxxZ4ZmBzP/547qM79tm7dyvz583nmmWe49NJLeeONN3j++ed58skn6dOnD9999x033XQTn376Kbfddhu33XYbl19+OU8++eTRc8TExPDWW28RHx/PgQMHGDt2LOeddx73338/69atY/Xq1YCTqCrNmDGD119/nXvvvZd9+/aRnp7OiSeeyF133cUZZ5zBnDlzyM3NZfTo0UycOJFWrVp59f0BK3GYJmTu16kcKinj91MH/GRbu9jmXH9yTxZtyGDd3rwARGdCTY8ePRg+fDgAJ554IqmpqXzzzTdccsklDB8+nJ///Ofs27cPgKVLl3LJJZcAcMUVPw5JU1Xuuusuhg4dysSJE9m7dy8ZGRnHve6ll17Kf/7zHwBef/31o+ddtGgR999/P8OHD2fChAkUFxeze/dur//dYCUO00QUl5Yz77tdTBzQgd7tY2vc59qTU3hqyXbmfpPKg5cM83OEpiE8KRn4SvPmzY8+joyMJCMjg4SEhKOlBE/MmzePrKwsVqxYQVRUFCkpKXWOtejcuTOJiYmsXbuW1157jaeeegpwktAbb7xBv379GvYH1YOVOEyT8O7qdA4WlTJrfO2jZuNjojh/eGf+uyadvKJSP0ZnwkF8fDw9evQ4WhpQVdasWQPA2LFjeeONNwB49dVXjx6Tl5dH+/btiYqK4rPPPmPXrl0AxMXFUVBQe3vbjBkzeOCBB8jLy2PIkCEAnH322fzrX/862jtw1apV3v8jXZY4TJPw5qo0eia1YmzPtsfd76qx3ThSVsHbq/f6KTITTubNm8dzzz3HsGHDGDRo0NEG6ocffpiHHnqI0aNHs2/fPlq3bg3AlVdeyfLlyxk5ciTz5s2jf//+ACQmJjJ+/HgGDx7Mb3/7259cZ/r06bz66qtceumlR1/74x//SGlpKUOHDmXw4MH88Y9/9NnfGVZrjttCTqYm+/OKGXf/Ym47sw+/mti3zv0n/fMLElpE8/rscX6IztTXxo0bGTDgp+1UwayoqIgWLVogIrz66qvMnz/fp72ealPTe9eQhZysjcOEvffWpqMK5w5L9mj/aUOT+ecnW9ifV0zH1jE+js40BStWrOCWW25BVUlISGDOnDmBDqlRLHGYsLfwh30MSo6nV1LNjeLVnTO0Ew99vIWFP+xj1smNn0nUmFNOOeVoe0c4sDYOE9ZyDpWwak8uZw3s4PExvZJi6dM+lk831bRUjDHGEocJa0u2ZKEKp/eraYXi2p3evz3LduZw6EiZjyIzJnRZ4jBh7fPNmSS2imZI59b1Om5C3yRKyitYuj3bR5EZE7oscZiwVV6hfLEli9P6JhERUb+1CEamtKVVdCSfbbbqKmOq81niEJE5IpIpIuuqvPZaldUAU0WkxiGW7rYf3P2sf61pkHV78zhYVMpp/ZLqfWx0swjG9WrHkq1ZPojMGHjyySd58cUXAZg7dy7p6elHt11//fVs2LAhUKHVyZe9quYCjwEvVr6gqpdVPhaRfwDHmxTodFU94LPoTNj7bqdTzTSuV2KDjh/XK5FPNmaQnnuY5IQW3gzNGGbPnn308dy5cxk8eDDJyU6X8WeffTZQYXnEZyUOVV0C5NS0TZw1DC8F5vvq+sYs25lDz3ataB/XsLEYY3q0PXoeY6pKTU2lf//+zJw5k6FDhzJ9+nSKiopYvHgxI0aMYMiQIcyaNYsjR44AzjTpAwcOZOjQodxxxx0A3HPPPTz44IMsWLCA5cuXc+WVVzJ8+HAOHz7MhAkTWL58OU888QS/+93vjl537ty53HrrrQC8/PLLjB49+uiEiuXl5X77+wM1juMUIENVt9ayXYFFIqLAU6r6dG0nEpEbgRsBunXr5vVATWgqr1CW7cxh6pBODT7HgE7xxMU047ud2VwwonPdBxj/++BO2P+Dd8/ZcQhMub/O3TZv3sxzzz3H+PHjmTVrFg899BBPPfUUixcvpm/fvlx99dU88cQTXH311bz11lts2rQJESE3N/eY80yfPp3HHnuMBx98kJEjR/5k27hx43jggQcAeO2117j77rvZuHEjr732Gl9//TVRUVHcdNNNzJs3j6uvvtp778NxBKpx/HKOX9oYr6onAFOAm0Xk1Np2VNWnVXWkqo5MSqp/XbYJT5v3F5BfXMaYOuamOp7ICGF0Slu+22ElDvNTXbt2Zfz48QBcddVVLF68mB49etC3rzOtzcyZM1myZAnx8fHExMRw/fXX8+abb9KyZUuPr5GUlETPnj359ttvyc7OZvPmzYwfP57FixezYsUKRo0axfDhw1m8eDE7dvhvBUu/lzhEpBlwEXBibfuoarp7nykibwGjgSX+idCEg8r2jdE9Gta+UWlMz7Ys3pRJZn4x7eNt+pGg40HJwFecGve6NWvWjGXLlrF48WJeffVVHnvsMT799FOPr3PZZZfx+uuv079/fy688EJEBFVl5syZ/PWvf21o+I0SiBLHRGCTqqbVtFFEWolIXOVjYBKwrqZ9janNsp05dGnTgs6NbNSuTDzLUq3UYY61e/duli5dCsD8+fOZOHEiqampbNu2DYCXXnqJ0047jcLCQvLy8pg6dSoPP/xwjet1HG8a9Ysuuoi3336b+fPnc9llTv+iM888kwULFpCZ6XQXz8nJOToluz/4sjvufGAp0E9E0kTkOnfTDKpVU4lIsogsdJ92AL4SkTXAMuB9Vf3QV3Ga8KOqLN91kFEpDa+mqjQoOZ7mzSJYtTu37p1NkzJgwABeeOEFhg4dSk5ODrfffjvPP/88l1xyCUOGDCEiIoLZs2dTUFDAtGnTGDp0KKeddhr//Oc/f3Kua665htmzZx9tHK+qTZs2DBw4kF27djF69GgABg4cyH333cekSZMYOnQoZ5111tHVBv3BplU3YSc99zAn3f8p9543iJknpTT6fBc/8Q0Ab/zipEafyzReMEyrnpqayrRp01i3LrQqQ7w1rbqNHDdhZ80ep3QwrGuCV843vGsC6/bmUVpe4ZXzGRPqLHGYsLM6LZfoyAgGdIrzyvmGd03gSFkFm/bVvpSnaVpSUlJCrrThTZY4TNhZvTuXAcnxNG8W6ZXzDXdLLqvTrJ0jWIRTFbu/ePM9s8Rhwkp5hfLD3jxGeKmaCqBLmxa0i41mtTWQB4WYmBiys7MtedSDqpKdnU1MjHe6lNsKgCasbMsspKiknGFd6zeN+vGICMO7JrB6z0GvndM0XJcuXUhLSyMryyagrI+YmBi6dOnilXNZ4jBhpfLLfVgX75U4wKmu+mRjJnmHS2ndIsqr5zb1ExUVRY8etqRvIFlVlQkrq/fkER/TjB7tWnn1vIPdhaA2pOd79bzGhCJLHCasrE/PY0iX1h5PB+GpQcmtj57fmKbOEocJG2XlFWzaX3D0S96bkuKa0z6uuZU4jMEShwkjOw4coqSsgoGd4n1y/sGdW7PeEocxljhM+KgsDQxM9k3iGJQcz7asQopL/bdgjjHByBKHCRsb9uUT3SyCnl5uGK80KDme8gpl834bQW6aNkscJmxsSM+nf8c4mkX65mP9YwO5VVeZps0ShwkLqsr69DyftW+AM4I8PqaZ9awyTZ4lDhMW9ucXc7Co1GftG+CMIB+YHG8lDtPkWeIwYeFow7gPSxzgVFdt2p9PmU2xbpowSxwmLFQmjv4+ThwDO8VTXFpBanaRT69jTDDz5dKxc0QkU0TWVXntHhHZKyKr3dvUWo6dLCKbRWSbiNzpqxhN+NiwL5+UxJbENvft9Gv9OjprfFjPKtOU+bLEMReYXMPr/1TV4e5tYfWNIhIJ/BuYAgwELheRgT6M04SBDfvyfdq+Ual3+1giBDbvt3YO03T5LHGo6hIgpwGHjga2qeoOVS0BXgXO92pwJqwcOlLGruwiBnT0feKIiYokpV0rNmdYicM0XYFo47hFRNa6VVltatjeGdhT5Xma+1qNRORGEVkuIsttfv6maVtmIQB9O3pnqdi69O8YZ1VVpknzd+J4AugFDAf2Af+oYZ+apjWtdakvVX1aVUeq6sikpCTvRGlCylY3cfRpH+uX6/XrEM+unCKKSsr8cj1jgo1fE4eqZqhquapWAM/gVEtVlwZ0rfK8C5Duj/hMaNqaUUB0ZATd2rb0y/X6dYxDFbZmFPrlesYEG78mDhHpVOXphcC6Gnb7HugjIj1EJBqYAbzrj/hMaNqaWUjPpFY+m2qkOutZZZo6n/VdFJH5wASgnYikAf8LTBCR4ThVT6nAz919k4FnVXWqqpaJyC3AR0AkMEdV1/sqThP6tmYWMLxrTc1lvtGtbUtioiLYZInDNFE+SxyqenkNLz9Xy77pwNQqzxcCP+mqa0x1RSVl7Mk5zCUndq17Zy+JjBD6dohji/WsMk2UjRw3IW175iHAfw3jlfp1iLMSh2myLHGYkLY10/ny7tPBP11xK/XrGMeBwiNkFx7x63WNCQaWOExI25pZSFSk0D3RPz2qKlkDuWnKLHGYkLY1o4Ae7VoR5aceVZUqE4dVV5mmyBKHCWlbMwv9Xk0FkBTbnDYto45WlRnTlFjiMCGruLSc3TlFfm8YB2dRpz7t42wQoGmSLHGYkLUtsxBV6NPe/yUOgN4dYtmaWYhqrTPiGBOWLHGYkHV0csMO/i9xgNMFOO9wKQcKSwJyfWMCxRKHCVlbMwtoFiF0T2wVkOtXlnSsncM0NZY4TMjaklFISrtWRDcLzMe4j1vSqSz5GNNUWOIwIWtbZmFAGsYrtY9rTlxMM2sgN02OJQ4TkopLy9mVfSggXXEriQi928daVZVpcixxmJC0I+sQFer/Oaqq69M+1qqqTJNjicOEpB/nqAp04ojjQGEJOYesZ5VpOixxmJC0LbOQyAihR7vA9Kiq1NsayE0TZInDhKStGYV0T2xJ82aRAY2jsqrMEodpSixxmJC0JbMg4O0bAMmtW9AyOtIayE2T4rPEISJzRCRTRNZVee3vIrJJRNaKyFsiklDLsaki8oOIrBaR5b6K0YSmI2Xl7Mouom8Ae1RViohwelZZicM0Jb4sccwFJld77WNgsKoOBbYAvz/O8aer6nBVHemj+EyISj1QRHmF0jsIShyA0yXXxnKYJsRniUNVlwA51V5bpKpl7tNvgS6+ur4JX5VrfQdqcsPq+rSPY39+MfnFpYEOxRi/CGQbxyzgg1q2KbBIRFaIyI3HO4mI3Cgiy0VkeVZWlteDNMFna2YhEQI9kwLbo6qSNZCbpiYgiUNE7gbKgHm17DJeVU8ApgA3i8iptZ1LVZ9W1ZGqOjIpKckH0Zpgsy2zgO6JrYiJCmyPqkpH56yy6irTRPg9cYjITGAacKXWspCBqqa795nAW8Bo/0Vogt2WjMKgad8A6NKmJdHNIqxnlWky/Jo4RGQy8D/AeapaVMs+rUQkrvIxMAlYV9O+pukpKasg9cChoOiKWykyQuiV5CzqZExT4MvuuPOBpUA/EUkTkeuAx4A44GO3q+2T7r7JIrLQPbQD8JWIrAGWAe+r6oe+itOEll3Zhyir0KDoiltVH+tZZZqQZr46sapeXsPLz9Wybzow1X28Axjmq7hMaKv8VR9MVVXgJI5316RTVFJGy2if/bcyJijYyHETUrZkFCACvZKCLHG4DeTbMw8FOBJjfM8ShwkpWzML6dqmJS2ig6NHVaXe7piSyjEmxoQzSxwmpGzLKKRvgKdSr0n3xJZERQrbsqydw4Q/SxwmZJSWV7DjQOHRX/fBJCoygh7tWrHVShymCbDEYULGruwiSss1qLriVtWnfZx1yTVNgiUOEzK2uQPsgq0rbqXe7WPZnVNEcWl5oEMxxqcscZiQscUdJ9GrfXDMUVVd3w5xqMJ2a+cwYc6jxCEi00TEkowJqK2ZhXRp0yJox0n0sWVkTRPhaTKYAWwVkQdEZIAvAzKmNlszCoK2mgogJbEVkRFiI8hN2PMocajqVcAIYDvwvIgsdaczD97/xSaslJVXsCMruOaoqi66WQQpiS1tLIcJex5XP6lqPvAG8CrQCbgQWCkit/ooNmOO2p1TREl5RdBNNVJdn/ZxVlVlwp6nbRznichbwKdAFDBaVafgzCl1hw/jMwb4cY6qPkFcVQVOO0dq9iGOlFnPKhO+PG1lnA78010O9ihVLRKRWd4Py5hjbQvSyQ2r69MhjgqFnQcO0b9jfKDDMcYnPK2q2lc9aYjI3wBUdbHXozKmmi0ZBXROaEFs8+DsUVWpsg3GGshNOPM0cZxVw2tTvBmIMcezNchW/atNj3atiBBsBLkJa8f9+SYivwBuAnqJyNoqm+KAr30ZmDGVyiuU7VmFnNQrMdCh1CkmKpLuiTZnlQlvdZX7XwE+AP4K3Fnl9QJVzfFZVMZUsTuniCNlFUE9hqOq3u1tGVkT3uqqqlJVTQVuBgqq3BCRtsc7UETmiEimiKyr8lpbEflYRLa6921qOXayiGwWkW0icmdN+5imo/LXe58gnE69Jn3ax5J64BAlZRWBDsUYn6grcbzi3q8Alrv3K6o8P565wORqr90JLFbVPsBiji3FACAikcC/cdpQBgKXi8jAOq5lwliodMWt1KdDLGUVyq5sWw3QhKfjJg5Vnebe91DVnu595a1nHccuAapXZ50PvOA+fgG4oIZDRwPbVHWHqpbgDDg834O/xYSpUOlRVamPu16IVVeZcOXpAMDxItLKfXyViDwkIt0acL0OqroPwL1vX8M+nYE9VZ6nua/VFtuNIrJcRJZnZWU1ICQT7LZkFIZMNRU466GLWJdcE7487Y77BFAkIsOA3wG7gJd8FJPU8JrWtrOqPq2qI1V1ZFJSko9CMoFS2aMqmOeoqq5FdCRd27RkS6b1rDLhydPEUaaqilNl9IiqPoLTJbe+MkSkE4B7n1nDPmlA1yrPuwDpDbiWCQO7c4ooKasImfaNSn3ax7LNShwmTHmaOApE5PfAVcD7bgN2VAOu9y4w0308E3inhn2+B/qISA8RicaZ0v3dBlzLhIHKmWZDpStupd4dYtlxoJCycutZZcKPp4njMuAIcJ2q7sdpc/j78Q4QkfnAUqCfiKSJyHXA/cBZIrIVZzT6/e6+ySKyEEBVy4BbgI+AjcDrqrq+3n+ZCQtHu+KGUFUVOA3kpeXKrpyiQIdijNd51E3FTRYPVXm+G3ixjmMur2XTmTXsmw5MrfJ8IbDQk9hMeNuSUUjnhBa0CpEeVZWqzlnVKym0kp4xdfG0V9VF7qC9PBHJF5ECEcn3dXDGbMkoCKkeVZUq59XaZg3kJgx5WlX1AHCeqrZW1XhVjVNVmzPa+FRZeQU7DhwKufYNgFbNm9E5oQVbrIHchCFPE0eGqm70aSTGVHO0R1WItW9U6tPB5qwy4cnTiuPlIvIa8DZOIzkAqvqmT6IyBo7+Wg/FEgc47RzfbM+mvEKJjKhpeJIxocnTxBEPFAGTqrymgCUO4zOVPapCYR2OmvRpH0dJWdTjQx8AACAASURBVAV7copIadcq0OEY4zWe9qq61teBGFPd1sxCurQJvR5VlXq7jfpbMwstcZiw4mmvqr4isrhyinQRGSoif/BtaKap25JRELLtG1ClS671rDJhxtPG8WeA3wOlAKq6FmdEtzE+UVZewY6s0OxRVSkuJopOrWPYst8ShwkvniaOlqq6rNprZd4OxphKu3KKKCkPvTmqquvXMY5NljhMmPE0cRwQkV64s9SKyHRgn8+iMk3eZvfLtn/H0E4c/TvGsz2rkFKbs8qEEU9bHW8Gngb6i8heYCdwpc+iMk3epn35REjo9qiqNKCTM2fVjqxD9AvxJGhMpeMmDhH5dZWnC4HPcEoph4CLqTJ/lTHetHF/AT2TYomJigx0KI1SmSw27c+3xGHCRl1VVXHubSTwC6ANkADMxlkP3Bif2LQ/P+SrqQB6toslKlLYuM/aOUz4OG6JQ1XvBRCRRcAJqlrgPr8H+I/PozNNUkFxKXtyDjNjVENWJw4u0c0i6JUUy6b9NieoCR+eNo53A0qqPC8BUrwejTH8uHhTOJQ4AAZ0imeTlThMGPG0cfwlYJmIvIXTs+pC4AWfRWWatMpqnf6dwmMC5v4d43hr1V5yi0pIaBkd6HCMaTSPShyq+hfgWuAgkAtcq6p/bcgFRaSfiKyucssXkV9V22eCu/ZH5T5/asi1TGjatD+fuJhmJLeOCXQoXvFjA7mVOkx48HgSIFVdCaxs7AVVdTMwHMBdu3wv8FYNu36pqtMaez0TejbtK2BAx3hEwmNG2QFuyWnTvnzG9kwMcDTGNJ6nbRy+ciawXVV3BTgOEyRUlU37C8Kq62r7uOa0aRllJQ4TNgKdOGYA82vZNk5E1ojIByIyqLYTiMiNIrJcRJZnZWX5JkrjN2kHD1N4pIz+ncIncYgI/TvGs9EShwkTAZuvWkSigfNwJk+sbiXQXVULRWQqzgJSfWo6j6o+jTOqnZEjR6qPwjV+sunoVCMh1DCuCgd3QtoKOLAFcrbDwV1QnAvFeVB6mDnlQlGZoI+2Q2I7QlxHaNcHOgyCDoOhbU8Ik6o5E/4CudDBFGClqmZU36Cq+VUeLxSRx0Wknaoe8GuExu827XP+6YO+qqooB7Yugs0fwK6v4ZBb2pUIaN0V2qRAQjeIaQ1RLUnbf5DvtmVwYWIrWpVkQ/oq2PA2qDuHVVwn6HEq9Dwd+k2BFgkB+9OMqUsgE8fl1FJNJSIdcdY5VxEZjVOllu3P4ExgbNpfQLe2LYkNxsWbSg/Dxv/Cqpcg9SvnSz+2I/SeCF1GQdfR0K4vNGv+k0OL9uTyh01fkzj0BKYM6fTj+bI2OUkk9SvY/imsfQ0ioqD3mTB4Ogw4F6LCo3eZCR8B+d8pIi2Bs4CfV3ltNoCqPglMB34hImXAYWCGqlo1VBOwYV8QTjVycBd8+zisme9UPSV0h1N+45QMOo2AiLqbCvt2iCNCYOO+/B8TR1QLSB7h3EbOcqq80lfC+rdg/duw5UNomQgnXO1sTwj9kfQmPAQkcahqEZBY7bUnqzx+DHjM33GZwCooLmXngUNcNKJzoENx7F8HXz8M6950qqAGnu98iaec4lGyqKpFdCS928eyLv04U4+IQOcTndtZ/w92fgHLnoGvH4GvH4VhM5yEldirkX+YMY0ThPUBpqna4H6pDu7cOrCBHNwFn94HP7wO0bEw9hcw9iZo3biENrhza77c6mEznQj0nODccvc4JZ7lc2DNqzD0MjjjbmjdpVHxGNNQljhM0FgX6MRxOBe+eAC+f8YpYZx8O4y/DVq08crpBye35s2Ve8nML6Z9fD3aLRK6wuS/wvhfwTePwvfPOtVZJ93qxNc8tNcsMaEn0OM4jDlq3d48OsQ3Jynup43LPqUKPyyAx0bBd0/A0Evh1pUw8R6vJQ34MSGuS89r2AniOsDZf4Fbvof+U2HJA/CvE2HdG87fYIyfWOIwQWPd3jwGJ/u5tJG9HV66EN64zqmKuuEzOP/fja6WqsnA5HhEYN3eRk6xntANps+BWYucZLJgFsy/HPL2eidQY+pgicMEhaKSMrZnFTLIX9VUFRXw3dPwxEmwdwVMfRCuXwzJw312ydjmzejRrhU/7G1giaO6bmPg+k+dhvQdn8PjY512ECt9GB+zxGGCwsZ9BVQoDE72w4jx/HR4+SL44LdOD6mbl8HoGyDC98vUDk5uzXpvJQ6AyGYw/pdw0zdO0nvvdnjlMii06XeM71jiMEFhvVvv7/OG8XVvwuPjYM93MO2fcOV/IL6Tb69ZxeDO8aTnFZNdeMS7J27bE65+F6Y84JQ+njgJtn7i3WsY47LEYYLCur15JLaKppOv1uAoOwLv/wYWXAuJvWH2V86gOj/PD1WZGNcfbzxHQ4nAmJ/DjZ9Bq3Yw72L48PdQVlL3scbUgyUOExTW7c1nUOfWvlmD4+AumDPZ6cZ60q0w68OADaIb5Db+e62doyYdBsENn8LonzvjP+ae41TPGeMlljhMwB0pK2dLRoFv2je2fARPnQrZ2+Cyl2HSfRAZ5f3reKh1iyi6tW15tGrOZ6JawNQH4JK5kLHeeQ92funba5omwxKHCbgN6fmUVShDvNm+UVEBn/0VXrnUGUD38y+cCQODwJDOrVmb5uPEUWnQhU7VVYs28OL5zvQl1uvKNJIlDhNwq/fkAjCim5cG25UcggXXwBf3w7Ar4LqPncbjIDGsa2vSDh7mgLcbyGuT1M+puhowDT7+E7x5A5QW++faJixZ4jABt2p3Lh3jY+jojYbxvDSnPWPDu0611AWPO9U2QaQyQa7eneu/izaPg0tegDP+CD/8x2n3KPjJUjjGeMQShwm41XtyGd7VCwsX7fkenj4dcnbCFa87DeFBuKre4OTWNIsQVu056N8Li8Cpd8ClL0HmBnjmDNj/g39jMGHBEocJqOzCI+zOKWJEt0YmjrXur+jolnD9J9B3kncC9IEW0ZEM6BTPKn+WOKoaeB5c+4GzENVzZ8OmhYGJw4QsSxwmoCrbNxpc4lB1GnzfvN5Zhe+Gz6B9fy9G6BvDuyawZk8u5RUBaqhOHu60eyT1hVevgKX/DkwcJiRZ4jABtXpPLpERwpAuDehRVVHhDHD7+E9O76GfvQkt23o/SB8Y0S2BQyXlbMssDFwQ8Z3gmoVOCeSju5z3sqIicPGYkBGQxCEiqSLyg4isFpHlNWwXEXlURLaJyFoROSEQcRrfW70nl34d4mgZXc+lYcqOwBuznGnQx94EF8+pca3vYFXZQL5qt5/bOaqLbgnT58KYXziDBRdcaz2uTJ0CWeI4XVWHq+rIGrZNAfq4txuBJ/wamfGLigp1Gsbr275RnAcvX+wsZnTW/4Oz/6/eS7kGWkpiSxJaRgWunaOqiAiYcj9M+gtseNuZAPJwgBOaCWrB+r/tfOBFdXwLJIiI/2aiM36x40AhBcVl9WvfyE+HOVNg91K46BlnZtgg7DlVFxFhRNcE//esOp6TbnHW+Uj73mk0z90T6IhMkApU4lBgkYisEJEba9jeGaj6qU1zX/sJEblRRJaLyPKsLJtKOpQs2+l8aY5K8bBdImszPDcJcnc5s9oOvdSH0fne8K5t2JpZSH5xaaBD+dHgi+GqN6FgPzw70brrmhoFKnGMV9UTcKqkbhaRU6ttr+knZI3dT1T1aVUdqaojk5KSvB2n8aHvU3NoF9uclMSWde+8+1snaZQdgWsXQq8zfB+gj41KaYMqrEgNolIHQI9T4LqPnPVJ5kxxpmk3poqAJA5VTXfvM4G3gNHVdkkDulZ53gWw6T3DzLKdOYzp0bbuGXE3vufMs9QyEa7/GDoN80+APjaiWxuiIoVvd2YHOpSfaj/AGQ+T0A1enu6MkzHG5ffEISKtRCSu8jEwCVhXbbd3gavd3lVjgTxV3efnUI0PpR0sYm/uYUal1DE/1ffPwes/gw6DnTmn2qT4JT5/aBEdydAuCSzbmRPoUGoWn+yU7rqNdcbJfPOvQEdkgkQgShwdgK9EZA2wDHhfVT8UkdkiMtvdZyGwA9gGPAPcFIA4jQ9VflmO7pFY8w6q8Ol98P6vofdZMPNdaFXLviFsTI+2/JCWR1FJWaBDqVmLBLjqDWeczKI/wId32VgPQz07zzeequ4AflLXoKpPVnmswM3+jMv41/epOcTFNKNfx7ifbiwvg/dug1Uvw4irYNojztraYWh0j7Y8/vl2Vu7K5eQ+7QIdTs2aNXfGycR2hG//DQX74MInQ2rcjPGuYO2Oa8LcdztzGJXSlsiIau0bJYecKTBWvQyn/g7OeyxskwbASPc9+C4Y2zmqioiAyX91xs2sf9MZR1PspzVFTNCxxGH8LqvgCDuyDjG6R7VuuIcOwAvnwraPYdo/4Yy7Q3KMRn3ENm/G4OR4vtsRpO0cVYk442YuesYZRzNnii1J20RZ4jB+9832AwCM7VmlzSJnp9PdNmO9s8TryFkBis7/Rvdoy+o9uRSXlgc6FM8MvdQZR5O7y/k3y9oc6IiMn1niMH731dYDtG4R9eNSsemr4Lmz4HAOXP0O9D8nsAH62dieiZSUV7ByV5CN5zieXmc4Pa7KjjjJY/e3gY7I+JElDuNXqspX2w5wUq9Ep31j6yfw/DnQrAXMWuR0/WxixvRMpFmE8MXWEJv5oNMwZ1xNq3bOOJuN7wU6IuMnljiMX23POsS+vGKnB9GqeTD/Mkjs6XwBJfUNdHgBEdu8GSd2b8OSLQcCHUr9tUlxEn6Hwc54m++fDXRExg8scRi/+nJrFqCcc3AevHMTpJzsrAkR1zHQoQXUqX2T2Lgvn8yCEJzSvFWiM86mzyR4/zew+P8543BM2LLEYfzqmy0ZPBL7Ignf/g2GXgZX/Adi4gMdVsCd1teZZ+3LUCx1AES3gsvmwQlXw5cPwju3QHkQTd5ovMoSh/GbksOFXJ56N+eXfQQn3w4XPgXNogMdVlAY2CmexFbRLAm1do6qIpvBuY/CaXfC6pdh/uVwJIArHBqfscRh/KNgP0eencIEVrBxxJ9g4j1hP0ajPiIihFP6tOPLrQeoCNQ65N4gAqf/Hs59BLYvhhemQWEIJ0NTI0scxvf2/wDPnEnzg1u5ueI3dJ9yW6AjCkqn9Usi51AJa/eGwYjsE6+BGa9A5iaYMwmytwc6IuNFljiMb23+EOZMRrWCG6P+QkmvyfVfX7yJOL1feyIjhEXr9wc6FO/oN8VpND+cC8+eCTuXBDoi4yWWOIxvqMLSx+HVyyGxN9sveIfP8zoycWCHQEcWtBJaRjOmR1s+CpfEAdB1tLOuR6v28NKFsHxOoCMyXmCJw3hfeakzHfpHv3dGgV+7kA93Oe0ZZ/ZvH+DggtvZgzqyPesQ2zLDqFE5sZczTqfn6fDe7bDwt84MyCZkWeIw3lWY5YwiXj4Hxv8KLnkRoluxaEMGw7om0D4+JtARBrWz3BLZog1hVOoAiGkNV7wG426BZU/DvIvhcAhNsWKOYYnDeM/eFfD0abB3pTOD6ln3QkQEu7IPsTYtj3OGNO1Bfp5ITmjB0C6t+Wh9RqBD8b6ISDj7L3D+vyH1a3jmTJsgMURZ4jDeseplZ5rtiEi4bpEzg6rrv2ucqbfPGZocqOhCytmDOrJmTy57cooCHYpvjLgKZv4XjuTD06fDujcCHZGpp0CsOd5VRD4TkY0isl5EftI3U0QmiEieiKx2b3/yd5zGQ2UlzjQT79wM3cfBjV9Ap6HH7PLumnRGpbShc0KLAAUZWs4b5iTYd9eE8VoX3cfBz5dAx8GwYBZ8cKfzWTIhIRAljjLgN6o6ABgL3CwiA2vY70tVHe7e/uzfEI1HDqbC85Odie3G3wZXvgEtj12cafP+ArZkFHLuMCtteKpr25aMTmnLmyvT0HCe8yk+Ga55H8beBN894QwWtIWhQoLfE4eq7lPVle7jAmAj0NnfcZhGWv82PHkqHNgGl74EZ/25xiVe3169lwiBKYM7BSDI0HXhCZ3ZnnWIH8JhMODxREY5S9JOf95ZxOvJU2D7p4GOytQhoG0cIpICjAC+q2HzOBFZIyIfiMig45zjRhFZLiLLs7JsagOfKy2G934N/5kJ7frA7CUw8Lyady2vYMGKNCb0a09SXHM/Bxrapg7uRHRkBG+u3BvoUPxj8EVww2fO2h4vXQgf3e0sEmWCUsASh4jEAm8Av1LV/GqbVwLdVXUY8C/g7drOo6pPq+pIVR2ZlJTku4CNM33EsxNh+XNw0i9h1ofOegy1+GxTJlkFR5gxqqv/YgwTrVtGcdbADry9em/oLCnbWEl9neQx6npY+pjzWcvaEuioTA0CkjhEJAonacxT1Terb1fVfFUtdB8vBKJEpJ2fwzSVKsrhm3/BU6dCQbozFfqk/+dUMxzHq9/voX1cc86wQX8NcuWYbuQWlfL+2n2BDsV/olvCOf+AGfMhf6/zmVv+vK3vEWQC0atKgOeAjar6UC37dHT3Q0RG48SZ7b8ozVE5O2HuNFj0B+g9EW76FvpOqvOwvbmH+XxzJpeM7EKzSOv13RDjeiXSK6kVL367K9Ch+F//qfCLb5ylhN/7FbxyKeQ1kWq7EBCI/9HjgZ8BZ1TpbjtVRGaLyGx3n+nAOhFZAzwKzNCw7l4ShCoq4Pvn4InxkLEOLngSZsyDWM9KD3O/3omIcPnobj4ONHyJCD8b2501e3JZm5Yb6HD8L64jXPUmTHkAUr+Cx8fCirlW+ggCEk7fxyNHjtTly5cHOozQl7HB+ZW35zvocZoz0jfB83aK/OJSTvrrp5zRvz2PXj7Ch4GGv/ziUsb+32ImDezAwzOa8HuZsxP++0tnht0ep8F5jx63fc14TkRWqOrI+hxjdQjmRyVF8Mm98NQpcGArnP84XP1OvZIGwGvL9lB4pIwbTunpo0CbjviYKK4c041316SzK/tQoMMJnLY94Op3YdrDzpQ2/x4LS/5uPa8CxBKHcYr+mz+AJ8bBVw85a4HfshxGXFnvVfqKS8t59qsdjOuZyJAurX0UcNNywyk9aRYZwZNfNPHFkERg5LVw87fQ5yz49D6n+mrLokBH1uRY4mjq9q9zZrOdPwMio2Hme3DB49AqsUGne2npLjLyj/CriX28HGjT1T4+hstGdmXBijT25h4OdDiB17oLXPYS/OwtkEh45RJ4ZYatMuhHljiaqsJMePeXTrXU/rVOA+QvvoEepzT4lAXFpTz++TZO7ZvEmJ4NSzymZrMn9EJE+Mcim032qF5nOJ/Zs/7stH38ezS8f4fz2TY+ZYmjqTmUDR//LzwyDFbPgzGz4daVMObndY7LqMsTn2/nYFEpd0zq66VgTaXOCS247uQevLlyb9PsYVWbZtHOPGm/XAknzHTWgXlkOHz2VzhSEOjowpYljqbi8EGnTviRofD1I9B/Gty8zJknqNrEhA2xLbOQZ77cwcUndGFolwQvBGyqu2lCLxJbRfPn/26goiJ8ekN6RVxHmPaQ85nucxZ8cT88PAS++Luz5rnxKksc4S5vrzN47+GhTi+UykF8Fz/jLOnpBarKn95ZR4uoSH4/tb9Xzml+Ki4miv+Z3J/luw4yb9nuQIcTnNr1hktfgBs+ha5j4bP7nASy+M9w6ECgowsbP53O1ISH/eucaULWLXB6TQ26AE6+HToO8fqlXly6i2+2Z/OXCwfTLtYmM/SlS0Z24b9r07l/4UYm9E2ia9uWgQ4pOHU+Ea54FfathS//AV8+BEsfdxYYG/Nz6FDrvKnGAzYAMJyUFsPGd525fXZ/A1Gt4ISfOesdtOnuk0tu3l/AuY99xfheicy5ZhRSz+67pv725h5m0kNf0K9jHK/eOI7oZlZxUKesLbD0X7D2P1B2GLqf7CSQflNrXA6gKWnIAEBLHOEgcxOsfBHWvOK0ZbTpASdeAydc7ZX2i9rkFZVy4RNfk3+4lA9uO9WmTvej99amc8srq7jmpBTuOc9+PXusKAdWvQTLnoW83RDb0SmFDL8C2g8IdHQB0ZDE0bRTbSjL3eOs1fzDAsj4ASKaQf9z4MRrnSkZInz7K7S0vIJfzFvBnpwiXr5ujCUNP5s2NJmVu3KZ8/VOerRrxcyTUgIdUmho2dbphTXuFtjyIax6Gb59HL55FJJHwLArYOD5ENch0JEGNStxhJLs7c4I703vwe6lzmudR8KQ6TD4Yo8nIGyskrIKbp2/ko/WZ/CPS4Zx8Yld/HJdc6yy8gpmv7ySTzZm8NClw7joBPt3aJDCLPjhP7D6FedHGOLMyjvgXOeWEN4TdVpVVbgljrIjsGeZ88toy4eQvc15vf0gGHyhkyza+nc+qENHyvjl/FUs3pTJ/547kGvH9/Dr9c2xikvLufb57/l2Zzb3nDvISh6NlbEBNv7XaSvMWOe81nEI9DrTGXDYbSw0C6/StSWOUE8cZUdg7wpnCunUL52kUVbsTAWScjL0nQJ9z/ZZQ3dd9uQUccOLy9mSUcC95w/mZ2MDE4c51uGScm6dv4pPNmYwc1x3fj91ADFRkYEOK/Rlb3eSyNaPYc+3UFEGUS2h+3hnhoWuYyF5eMgnEkscoZQ4KiogZzukr3Jm+0xfBftWO4kCgY6DIeUUJ2H0OBWaxwUwVGXest3cv3AjkRHCY1ecwKl9bZneYFJeofzl/Y3M+Xon/TvG8eAlwxjc2SaZ9JojBc4Puu2fOrfK0n9ktNM20nU0dBnllE4SUnzexuhNljiCMXGoQmEGZG2CrM3ubRPs/wGOuEutR7WEjkOdvucp46HbOJ/2hvKUqvLFliwe+ngLa9PyGN87kfsvGmpjB4LY4o0Z/M8ba8k+VMIlJ3bh1jP62L+XLxRmOuvV7PnOqRlIXwXlJc626FjoMNhJIh0HQ7u+kNgbWiXVe7Zpf7DEEajEUVIEeWmQu8u97YaD7n32djiS9+O+Ma0hqb/zoUoeAcknOB+sIOpLnne4lHdW72X+sj1s3JdP54QW/Pqsvlx0QmcbpxEC8otL+dfircz9JpXyCmXK4E7MGN2Vk3q1IzLC/v18orQYMjc4Pwgz1jn3+9dBSZX5sqLjnNkaEns7bZOtu0DrzhDv3geoViFkEoeITAYeASKBZ1X1/mrbxd0+FSgCrlHVlXWd1yuJo6IcivN+ejuc4/S+OJTplCAKs5z7Q1k/lhwqRUY7PTESujljKpL6Q1I/5z62fVD+6qhq5pxlfLEli0HJ8Vw5pjvTT+xig8xC0L68w8z9JpX53+0mv7iMBy8ZxnTrAec/FRXOD8ns7U61dPY253H2NsjbA1px7P7NWzsJpFUStGoHLdu594k/Pm/ZFprHQ0y8U7LxwndJSCQOEYkEtgBnAWnA98Dlqrqhyj5TgVtxEscY4BFVHVPXuRuUOFSdqcWLDjoJoqSOGTVjWkNsB2jV3kkCsR0gNglad/sxWcR2CKk6zupW78klUsQWYgoTxaXlfLopk5P7tCM+pnEzIBsvKS+Fgn3OXHL5e50ai/y9zvNDWVB0wJnJumptRXUS4ZRiYuKd0susDxsUSqgMABwNbFPVHQAi8ipwPrChyj7nAy+qk9W+FZEEEemkqvu8Ho0IJA1wphSPaX2cW4LzSyAqxushBJvhXW1223ASExXJ1CGdAh2GqSoy6scfmsdTVgJF2W4iOeDMDHEkH4rzj72P8O9XeSASR2dgT5XnaTilirr26Qz8JHGIyI3AjQDdujVwoM7FzzTsOGOM8aVm0RDfybkFkUDUp9RUKVe9vsyTfZwXVZ9W1ZGqOjIpybqIGmOMrwUicaQBXas87wKkN2AfY4wxARCIxPE90EdEeohINDADeLfaPu8CV4tjLJDnk/YNY4wx9eb3Ng5VLRORW4CPcLrjzlHV9SIy293+JLAQp0fVNpzuuNf6O05jjDE1C8ioM1VdiJMcqr72ZJXHCtzs77iMMcbULXQHGxhjjAkISxzGGGPqxRKHMcaYegmrSQ5FJAvY5efLtgMO+PmangjGuIIxJgjOuIIxJgjOuIIxJgjOuGqKqbuq1msQXFgljkAQkeX1nefFH4IxrmCMCYIzrmCMCYIzrmCMCYIzLm/FZFVVxhhj6sUShzHGmHqxxNF4Twc6gFoEY1zBGBMEZ1zBGBMEZ1zBGBMEZ1xeicnaOIwxxtSLlTiMMcbUiyUOY4wx9WKJwwMi0lZEPhaRre59m1r2mywim0Vkm4jcWcP2O0RERaRdMMQlIn8XkU0islZE3hKRBi/958HfLiLyqLt9rYic4Omx/o5JRLqKyGcislFE1ovIbd6KqTFxVdkeKSKrROS9YIjJXaFzgftZ2igi44Ikrtvdf791IjJfRLyyfKcHMfUXkaUickRE7qjPsYGIq0Gfd1W1Wx034AHgTvfxncDfatgnEtgO9ASigTXAwCrbu+LMCLwLaBcMcQGTgGbu47/VdLyHcRz3b3f3mQp8gLNI11jgO0+PDUBMnYAT3MdxwBZvxNTYuKps/zXwCvBeMMQEvABc7z6OBhICHRfOiqE7gRbu89eBa/wUU3tgFPAX4I76HBuguOr9ebcSh2fOx/nPgXt/QQ37HF1LXVVLgMq11Cv9E/gdtaxkGIi4VHWRqpa5+32Ls2BWQ9T1t1fG+qI6vgUSRKSTh8f6NSZV3aeqKwFUtQDYiPNF5A2Nea8QkS7AOcCzXoqnUTGJSDxwKvAcgKqWqGpuoONytzUDWohIM6Al3lkMrs6YVDVTVb8HShvw9/g9roZ83i1xeKaDugtJuffta9intnXSEZHzgL2quiaY4qpmFs4vt4bw5Bq17eNpfP6M6SgRSQFGAN95ISZvxPUwzg+QCi/F09iYegJZwPNu9dmzItIq0HGp6l7gQWA3sA9nMbhFforJF8f65dyeft4tcbhE5BO3LrT6zdNfBDWuky4iLYG7gT8FU1zVrnE3UAbMa0iMnlzjOPt4vL58PTV6bXsRiQXeAH6lqvleiKlRcYnINCBTVVd4L7O7VgAAAzRJREFUKZZGx4Tzq/4E4AlVHQEcwqk2DWhc4rT3nQ/0AJKBViJylZ9i8sWxPj93fT7vAVnIKRip6sTatolIRmUVhlsMzqxht9rWSe+F8+FdIyKVr68UkdGquj+AcVWeYyYwDThT3UrOBmjMOvLRHhzr75gQkSic/0TzVPVNL8TjjbimA+eJyFQgBogXkZdVtbFfiI2JSYE0Va38hboA7yWOxsQ1EdipqlkAIvImcBLwsh9i8sWxPj13vT/v3miYCfcb8HeObYR+oIZ9mgE7cJJEZePUoBr2S8V7jeONiguYDGwAkhoZR51/O069fNVGzGX1ed/8HJMALwIP++Cz1OC4qu0zAe81jjcqJuBLoJ/7+B7g74GOCxgDrMdp2xCcNsBb/RFTlX3v4dhGaJ981r0QV70/7179TxGuNyARWAxsde/buq8nAwur7DcVp0fCduDuWs6VivcSR6PiwlnTfQ+w2r092YhYfnINYDYw230swL/d7T8AI+vzvvkzJuBknF/Sa6u8N1MDHVe1c0zAS4nDC/9+w4Hl7vv1NtAmSOK6F9gErANeApr7KaaOOCWAfCDXfRzvy896Y+JqyOfdphwxxhhTL9Y4bowxpl4scRhjjKkXSxzGGGPqxRKHMcaYerHEYYwxpl4scRjTSCJSGOgYjPEnSxzGGGPqxRKHMV7irg3xd3cusR9E5DL39U4iskREVrvbTnHX1JhbZd/bAx2/MZ6yuaqM8Z6LcEZRDwPaAd+LyBLgCuAjVf2LiETiTIMxHGcW18HgLIYUoJiNqTcrcRjjPScD81W1XFUzgC9wFs75HrhWRO4Bhqiz5sEOoKeI/EtEJuNMA2FMSLDEYYz31DS1Naq6BGexo73ASyJytaoexCmZfA7cjHcXZjLGpyxxGOM9S4DL3PaLJJxksUxEuuOso/EMzkp5J4iz7nyEqr4B/BFnTQtjQoK1cRjjPW8B43CmtFbgd6r/v707tmIQiIEouGqULgioxu1RgDMHEAAFrBOSmQqU/XsXSMd+3zxZZ+aX5JtkyXWd7TMzz+Nte2Ng+IftuABUfFUBUBEOACrCAUBFOACoCAcAFeEAoCIcAFRO9Zx8TJMpCAYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Distribution of training loss')\n",
    "plt.xlabel('loss')\n",
    "plt.ylabel('density')\n",
    "plot_normal_distribution(np.mean(true_loss), np.var(true_loss), 0.5)\n",
    "plot_normal_distribution(np.mean(false_loss), np.var(false_loss), 0.5)\n",
    "plt.legend(['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03403441 0.00064003]\n"
     ]
    }
   ],
   "source": [
    "intersection = solve_intersection(np.mean(true_loss), np.mean(false_loss), np.var(true_loss), np.var(false_loss), 0.5, 0.5)\n",
    "print(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7125) tensor(0.7713) tensor(0.4164) tensor(0.5408)\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.cat((X_true_test, X_false_test), 0)\n",
    "y_test = torch.cat((y_true_test, y_false_test), 0)\n",
    "y_test_hat = test_model.forward(X_test)\n",
    "test_loss = criterion(y_test_hat, X_test)\n",
    "test_acc = binary_acc(torch.mean(test_loss, dim=1), y_test, intersection[0])\n",
    "precision, recall = get_precision_and_recall(torch.mean(test_loss, dim=1), y_test, intersection[0])\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "print(test_acc, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "\n",
    "# ... load data, define score function\n",
    "def score(X, y):\n",
    "    X = torch.from_numpy(X)\n",
    "    y = torch.from_numpy(y)\n",
    "    y_pred = test_model.forward(X)\n",
    "    test_loss = criterion(y_pred, X)\n",
    "    test_acc = binary_acc(torch.mean(test_loss, dim=1), y, intersection[0])\n",
    "    precision, recall = get_precision_and_recall(torch.mean(test_loss, dim=1), y, intersection[0])\n",
    "    precision = precision.numpy()\n",
    "    recall = recall.numpy()\n",
    "    return 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "base_score, score_decreases = get_score_importances(score, X_test.numpy(), y_test.numpy())\n",
    "feature_importances = np.mean(score_decreases, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FI = {'Features':  list(normalized_data.drop('TRANSACTIONCANCELLED', axis=1).columns.values),\n",
    "        'Feature Importances': list(feature_importances)}\n",
    "\n",
    "Feature_Importance = pd.DataFrame(FI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_FI = Feature_Importance.sort_values(by='Feature Importances', ascending = False)\n",
    "sorted_FI.reset_index(inplace=True)\n",
    "sorted_FI.drop(\"index\", axis = 1, inplace= True)\n",
    "sorted_FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_FI.head(32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
